{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchdata==0.3.0 in /root/.local/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.11.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.25.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchdata==0.3.0) (3.7.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import torchtext.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags to avoid repeat work\n",
    "\n",
    "get_embeddings = False\n",
    "get_input_data = True\n",
    "augment_embed_data = True\n",
    "preprocess_input_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "embedding_name = 'glove-twitter-100'\n",
    "validation_index = 87599 # Index where validation set starts\n",
    "sosToken = 'soseq'\n",
    "eosToken = 'eoseq'\n",
    "loader_qty = 1 # Data loading thread quantity\n",
    "layer_count = 1\n",
    "hidden_unit_dim = 512\n",
    "rep_int = 1000 # Samples per status printout\n",
    "val_int = 5000 # Batches per validation (with printout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embeddings\n",
    "\n",
    "if (get_embeddings) == True:\n",
    "    \n",
    "    base_embeddings = gensim.downloader.load(embedding_name)\n",
    "    base_embeddings.save(embedding_name+'.kv')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    base_embeddings = KeyedVectors.load(embedding_name+'.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_input_data == True:\n",
    "\n",
    "    train_squad, dev_squad = torchtext.datasets.SQuAD1()\n",
    "    base_data = []\n",
    "    for dP in train_squad:\n",
    "        for dpAns in dP[2]:\n",
    "            base_data.append((\" \".join([sosToken,dP[1],eosToken]),\" \".join([sosToken,dpAns,eosToken])))\n",
    "    \n",
    "    for dP in dev_squad:\n",
    "        for dpAns in dP[2]:\n",
    "            base_data.append((\" \".join([sosToken,dP[1],eosToken]),\" \".join([sosToken,dpAns,eosToken])))\n",
    "    \n",
    "    qa_df = pd.DataFrame(base_data,columns = ['qTxt','aTxt'])\n",
    "    qa_df.to_pickle(\"rawQuestAnsData.pkl\")\n",
    "\n",
    "else:\n",
    "    qa_df = pd.read_pickle(\"rawQuestAnsData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sequence boundary tokens, make all keywords lowercase, rebuild and save keyedVectors\n",
    "\n",
    "if augment_embed_data == True:\n",
    "    \n",
    "    # Prepare numpy array to hold new embedding matrix with sosToken and eosToken added as one-hot\n",
    "    aug_words = []\n",
    "    aug_embed = np.zeros((len(base_embeddings.index_to_key)+2,len(base_embeddings[0])+2))\n",
    "    for i in range(len(base_embeddings.index_to_key)):\n",
    "        aug_words.append(base_embeddings.index_to_key[i].lower())\n",
    "        aug_embed[i,:-2] = base_embeddings[base_embeddings.index_to_key[i]]\n",
    "    \n",
    "    # Add sosToken and eosToken\n",
    "    aug_words.append(sosToken)\n",
    "    aug_embed[-2,-2:] = np.array([1,0])\n",
    "    aug_words.append(eosToken)\n",
    "    aug_embed[-1,-2:] = np.array([0,1])\n",
    "    \n",
    "    # Create new KeyedVectors instance with the extra dimensions for sos, eos\n",
    "    aug_kv = KeyedVectors(aug_embed.shape[1],aug_embed.shape[0])\n",
    "    aug_kv.add_vectors(aug_words,aug_embed)\n",
    "    \n",
    "    # aug_kv.unit_normalize_all()\n",
    "    \n",
    "    # Save\n",
    "    aug_kv.save(embedding_name+'-aug.kv')\n",
    "    \n",
    "else:\n",
    "    aug_kv = KeyedVectors.load(embedding_name+'-aug.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "# Pre-Reqs\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Remove tokens from list not present in embedding\n",
    "def scrubTokens(inTokenList,emb_kv):\n",
    "    outList = [ token for token in inTokenList if (emb_kv.has_index_for(token)) ]\n",
    "    return outList\n",
    "\n",
    "# Tokenization\n",
    "def prepare_text(sentence,emb_kv):\n",
    "    tokens = scrubTokens(word_tokenize(sentence),emb_kv)\n",
    "    return tokens\n",
    "\n",
    "# Prepend to token list\n",
    "def token_prepend(inList,preItem):\n",
    "    return [preItem] + inList\n",
    "\n",
    "# Append to token list\n",
    "def token_append(inList,postItem):\n",
    "    return inList + [postItem]\n",
    "\n",
    "# Transform list of tokens to their indices in embedding\n",
    "def tokensToIndices(tokens,emb_kv):\n",
    "    tokenInds = [emb_kv.get_index(token) for token in tokens]\n",
    "    return tokenInds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text\n",
      "Pruned vocabulary\n",
      "Converted to indices\n",
      "Saved preprocessed data\n"
     ]
    }
   ],
   "source": [
    "# Tokenize strings\n",
    "# Remove tokens that are not in the embedding\n",
    "# Perform training and validation split\n",
    "# Prepare new KeyedVectors that does not have tokens absent from training data\n",
    "\n",
    "if preprocess_input_data == True:\n",
    "    \n",
    "    proc_qa_df = qa_df.copy()\n",
    "    proc_qa_df['qTxtClean'] = proc_qa_df['qTxt'].str.lower().apply(prepare_text, emb_kv = aug_kv)\n",
    "    proc_qa_df['aTxtClean'] = proc_qa_df['aTxt'].str.lower().apply(prepare_text, emb_kv = aug_kv)\n",
    "    \n",
    "    # Remove rows with no answers in vocab\n",
    "    proc_qa_df['aTxtLen'] = proc_qa_df['aTxtClean'].apply(len)\n",
    "    proc_qa_df = proc_qa_df[proc_qa_df['aTxtLen']>2]    \n",
    "    \n",
    "    print(\"Cleaned Text\")\n",
    "    \n",
    "    pr_kv = KeyedVectors(aug_kv[sosToken].size)\n",
    "    \n",
    "    keyDict = {}\n",
    "    for series in [proc_qa_df['qTxtClean'],proc_qa_df['aTxtClean']]:\n",
    "        for index,tokenList in series.items():\n",
    "            for token in tokenList:\n",
    "                if not token in keyDict.keys():\n",
    "                    keyDict[token] = aug_kv[token]\n",
    "    \n",
    "    keyList = []\n",
    "    valList = []\n",
    "    for token in keyDict.keys():\n",
    "        keyList.append(token)\n",
    "        valList.append(keyDict[token])\n",
    "    \n",
    "    pr_kv.add_vectors(keyList,valList)\n",
    "    \n",
    "    print(\"Pruned vocabulary\")\n",
    "    \n",
    "    # Add dataframe columns with tokens by their numerical indices\n",
    "    proc_qa_df['qIdxs'] = proc_qa_df['qTxtClean'].apply(tokensToIndices, emb_kv = pr_kv)\n",
    "    proc_qa_df['aIdxs'] = proc_qa_df['aTxtClean'].apply(tokensToIndices, emb_kv = pr_kv)\n",
    "    \n",
    "    print(\"Converted to indices\")\n",
    "    \n",
    "    # Save tokenized dataframes with training and validation split\n",
    "    train_qa_df = proc_qa_df[:validation_index]\n",
    "    train_qa_df.to_pickle('tokenizedTrainingData.pkl')\n",
    "    validation_qa_df = proc_qa_df[validation_index:]\n",
    "    validation_qa_df.to_pickle('tokenizedValidationData.pkl')\n",
    "    pr_kv.save(embedding_name+'-prn.kv')\n",
    "    print(\"Saved preprocessed data\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    train_qa_df = pd.read_pickle('tokenizedTrainingData.pkl')\n",
    "    validation_qa_df = pd.read_pickle('tokenizedValidationData.pkl')\n",
    "    pr_kv = KeyedVectors.load(embedding_name+'-prn.kv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object\n",
    "\n",
    "class qaWithContextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, questionAndAnswer_df):\n",
    "        self.qa_df = questionAndAnswer_df\n",
    "        self.length = questionAndAnswer_df['qIdxs'].count()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return tuple of (question,answer)\n",
    "        return (torch.tensor(self.qa_df['qIdxs'].iat[idx]),torch.tensor(self.qa_df['aIdxs'].iat[idx]))\n",
    "    \n",
    "# Batch collation - Pad sequences as tensors, add sequence lengths as lists with nested tuple\n",
    "# ( (questionTensor,questionLengthList), (answerTensor,answerLengthList) )\n",
    "def collate_qa_samples(batch):\n",
    "    \n",
    "    # Sort batch tuples by decreasing question length\n",
    "    batch_sorted = batch.copy()\n",
    "    batch_sorted.sort(reverse = True, key = lambda qa_p: qa_p[0].size()[0])\n",
    "    \n",
    "    questionLengths = [qa_pair[0].size()[0] for qa_pair in batch_sorted]\n",
    "    answerLengths = [qa_pair[1].size()[0] for qa_pair in batch_sorted]\n",
    "    \n",
    "    # Pad sequences with 1's.\n",
    "    # during forward() computations\n",
    "    questions = torch.ones([len(questionLengths),max(questionLengths)], dtype=torch.int64)\n",
    "    answers = torch.ones([len(answerLengths),max(answerLengths)], dtype=torch.int64)\n",
    "    \n",
    "    # Copy sequences into output tensor\n",
    "    for i in range(len(batch_sorted)):\n",
    "        questions[i,0:questionLengths[i]] = batch_sorted[i][0]\n",
    "        answers[i,0:answerLengths[i]] = batch_sorted[i][1]\n",
    "    \n",
    "    return ( (questions,questionLengths) , (answers,answerLengths) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and validation datasets\n",
    "# Truncate to 5000 to try and manage training time\n",
    "train_dataset = qaWithContextDataset(train_qa_df[0:5000])\n",
    "validation_dataset = qaWithContextDataset(validation_qa_df[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader creator so batch_size may be varied\n",
    "def make_dataloader(d_set,batch_qty):\n",
    "    return torch.utils.data.DataLoader( \\\n",
    "                       d_set, \\\n",
    "                       batch_qty, \\\n",
    "                       shuffle = True, \\\n",
    "                       num_workers = loader_qty, \\\n",
    "                       collate_fn = collate_qa_samples, \\\n",
    "                       drop_last = True, \\\n",
    "                       persistent_workers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_embed):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        adjusted_hidden = hidden_size if hidden_size > pretrained_embed.size(dim=1) else pretrained_embed.size(dim=1)\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding( \\\n",
    "                                     num_embeddings = pretrained_embed.size(dim=0), \\\n",
    "                                     embedding_dim = adjusted_hidden, \\\n",
    "                                     )\n",
    "        \n",
    "        # initialize weights for encoder embedding, loading pretrained, expand hidden size if less than pretrained dim\n",
    "        \n",
    "        init_weights = torch.randn(pretrained_embed.size(dim=0),adjusted_hidden)\n",
    "        init_weights[:,:pretrained_embed.size(dim=1)] = pretrained_embed\n",
    "        self.embedding.weight = torch.nn.Parameter(init_weights)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        if (layer_qty > 1):\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True,dropout=0.3)\n",
    "        else:\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src tuple (questions, questionLengths) for batch\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        # Shape of i[0] is [batch_size,sequence_length]\n",
    "        \n",
    "        max_question_length = max(i[1])-1 \n",
    "        \n",
    "        # Get permutation order for sorting questions by decreasing length\n",
    "        sorted_ques_lengths = []\n",
    "        for idx in range(len(i[1])):\n",
    "            sorted_ques_lengths.append((i[1][idx],idx))\n",
    "        sorted_ques_lengths.sort(reverse = True, key = lambda ansLen : ansLen[0])\n",
    "        \n",
    "        # Split up for easier downstream ops\n",
    "        sorted_dec_lengths = [quesLen[0] for quesLen in sorted_ques_lengths]\n",
    "        sorted_idx_list = [quesLen[1] for quesLen in sorted_ques_lengths]\n",
    "        \n",
    "        # Sort the answer padded tensor by decreasing answer length\n",
    "        sorted_idx_tensor = i[0].new_tensor(sorted_idx_list,dtype=torch.int64)\n",
    "        decreasing_length_questions = torch.index_select(i[0],0,sorted_idx_tensor)\n",
    "        \n",
    "        # Shape of embed_rslt is [batch_size,sequence_length,embedding_dim]\n",
    "        embed_rslt = self.embedding(decreasing_length_questions)\n",
    "        \n",
    "        # Encoder does not require online data substitution.  Can use sequence packing functionality.\n",
    "        packed_questions = torch.nn.utils.rnn.pack_padded_sequence(embed_rslt,i[1],batch_first=True,enforce_sorted=True)\n",
    "        \n",
    "        o,(h,c) = self.lstm(packed_questions)\n",
    "        \n",
    "        # Need to undo permutation so questions and answers remain aligned. Output of encoder is unused\n",
    "        inverted_sort_list = [sorted_idx_list.index(old_idx) for old_idx in range(len(sorted_idx_list))]\n",
    "        inverted_sort_tensor = i[0].new_tensor(inverted_sort_list,dtype=torch.int64)\n",
    "        h = torch.index_select(h,1,inverted_sort_tensor)\n",
    "        c = torch.index_select(c,1,inverted_sort_tensor)\n",
    "        \n",
    "        # h.shape == c.shape == [num_layers, batch_size, hidden_size]\n",
    "        return o, (h, c)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_embed):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        adjusted_hidden = hidden_size if hidden_size > pretrained_embed.size(dim=1) else pretrained_embed.size(dim=1)\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding( \\\n",
    "                                     num_embeddings = pretrained_embed.size(dim=0), \\\n",
    "                                     embedding_dim = adjusted_hidden, \\\n",
    "                                     )\n",
    "        \n",
    "        # initialize weights for encoder embedding, loading pretrained, expand hidden size if less than pretrained dim\n",
    "        init_weights = torch.randn(pretrained_embed.size(dim=0),adjusted_hidden)\n",
    "        init_weights[:,:pretrained_embed.size(dim=1)] = pretrained_embed\n",
    "        self.embedding.weight = torch.nn.Parameter(init_weights)\n",
    "        \n",
    "        # Output dimension used to construct outputs\n",
    "        self.out_dim = pretrained_embed.size()[0]\n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        if (layer_qty > 1):\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True,dropout=0.3)\n",
    "        else:\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True)\n",
    "        \n",
    "        # self.output, predicts on the LSTM output with linear layer\n",
    "        self.output = nn.Linear(adjusted_hidden,pretrained_embed.size()[0])\n",
    "        self.lsftmx = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, i, enc_state, teach_freq):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target tuple (answers, answerLengths) for batch\n",
    "        Outputs: o, the prediction\n",
    "        '''\n",
    "\n",
    "        are_teaching = True if random.random() < teach_freq else False\n",
    "        max_answer_length = max(i[1])-1 \n",
    "        \n",
    "        # Get permutation order for sorting answers by decreasing length\n",
    "        # Each answer length is reduced by one since the eos token is only used in loss calculation\n",
    "        sorted_ans_lengths = []\n",
    "        for idx in range(len(i[1])):\n",
    "            sorted_ans_lengths.append((i[1][idx]-1,idx))\n",
    "        sorted_ans_lengths.sort(reverse = True, key = lambda ansLen : ansLen[0])\n",
    "        \n",
    "        # Split up for easier downstream ops\n",
    "        sorted_dec_lengths = [ansLen[0] for ansLen in sorted_ans_lengths]\n",
    "        sorted_idx_list = [ansLen[1] for ansLen in sorted_ans_lengths]\n",
    "        \n",
    "        # Sort the answer padded tensor by decreasing answer length\n",
    "        sorted_idx_tensor = i[0].new_tensor(sorted_idx_list,dtype=torch.int64)\n",
    "        decreasing_length_answers = torch.index_select(i[0],0,sorted_idx_tensor)\n",
    "        \n",
    "        # Allocate tensor for predictions\n",
    "        out_preds = i[0].new_ones([len(i[1]),max_answer_length,self.out_dim])\n",
    "        \n",
    "        # Encoder states need to be permuted in same manner as answers (decreasing answer length)\n",
    "        h_enc_decreasing_ans_length = torch.index_select(enc_state[0],1,sorted_idx_tensor)\n",
    "        c_enc_decreasing_ans_length = torch.index_select(enc_state[1],1,sorted_idx_tensor)\n",
    "        \n",
    "        # Teaching: \n",
    "        #  -Can pack answer sequence and run full sequence with single lstm call\n",
    "        # Not Teaching: \n",
    "        #  -Step one token at a time, feeding previous prediction tokens as input.\n",
    "        #  -Don't run sequences through decoder beyond their labeled answer length\n",
    "        if are_teaching:\n",
    "               \n",
    "            embed_rslt = self.embedding(decreasing_length_answers[:,:-1]) # Do not pass eos token\n",
    "            packed_answers = torch.nn.utils.rnn.pack_padded_sequence( \\\n",
    "                embed_rslt,sorted_dec_lengths,batch_first=True,enforce_sorted=True)\n",
    "            teach_pred_out,(h_decode,c_decode) = self.lstm( \\\n",
    "                                                            packed_answers, \\\n",
    "                                                            (h_enc_decreasing_ans_length,c_enc_decreasing_ans_length) \\\n",
    "                                                          )\n",
    "            unpacked_ans, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(teach_pred_out,batch_first=True)\n",
    "            \n",
    "            # Pass through linear layer\n",
    "            out_preds = self.output(unpacked_ans)\n",
    "            out_preds = self.lsftmx(out_preds)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            prev_h = h_enc_decreasing_ans_length\n",
    "            prev_c = c_enc_decreasing_ans_length\n",
    "            \n",
    "            # feed in sos token on first iteration to prime\n",
    "            prev_o = torch.unsqueeze(self.embedding(decreasing_length_answers[:,0]),1)\n",
    "            \n",
    "            # Accumulator list and alias\n",
    "            out_pred_list = []\n",
    "            total_seq = len(sorted_dec_lengths)\n",
    "            \n",
    "            for step in range(max_answer_length):\n",
    "                \n",
    "                # Ensure prepped for gpu run\n",
    "                prev_h = prev_h.contiguous()\n",
    "                prev_c = prev_c.contiguous()\n",
    "                prev_o = prev_o.contiguous()\n",
    "                \n",
    "                # Determine number of sequences that still have remaining predictions to make\n",
    "                seq_left = sum([(1 if step <= ans_len else 0) for ans_len in sorted_dec_lengths])\n",
    "                ltsm_out, (prev_h,prev_c) = \\\n",
    "                    self.lstm( \\\n",
    "                        prev_o[:seq_left,:,:].contiguous(), \\\n",
    "                        (prev_h[:,:seq_left,:].contiguous(),prev_c[:,:seq_left,:].contiguous()))\n",
    "                net_out = self.output(ltsm_out)\n",
    "                net_out = self.lsftmx(net_out)\n",
    "                prev_o = self.embedding(torch.argmax(net_out,dim=2))\n",
    "                \n",
    "                # Add tensor with outputs for seq_left sequences padded \n",
    "                out_pred_list.append( \\\n",
    "                     torch.cat([net_out,i[0].new_ones([total_seq-seq_left,1,self.out_dim])], dim = 0) \\\n",
    "                )\n",
    "        \n",
    "            # Concatenate all the sequence outputs from every sequence step\n",
    "            out_preds = torch.cat(out_pred_list,dim=1)\n",
    "        \n",
    "        # Reorder predictions to original order for loss computation\n",
    "        inverted_sort_list = [sorted_idx_list.index(old_idx) for old_idx in range(len(sorted_idx_list))]\n",
    "        o = torch.index_select(out_preds,0,i[0].new_tensor(inverted_sort_list,dtype=torch.int64))\n",
    "        \n",
    "        return o\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_kv):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        # Convert keyedvector's numpy array to tensor\n",
    "        pretrained_embed = torch.tensor(pretrained_kv.vectors,dtype=torch.float32)\n",
    "        \n",
    "        self.seq2seqEncoder = Encoder(hidden_size, layer_qty, pretrained_embed)\n",
    "        self.seq2seqDecoder = Decoder(hidden_size, layer_qty, pretrained_embed)\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "        \n",
    "        o, enc_state = self.seq2seqEncoder.forward(src)\n",
    "        o = self.seq2seqDecoder.forward(trg, enc_state, teacher_forcing_ratio)\n",
    "        \n",
    "        return o\n",
    "    \n",
    "    # Pass tensor to embedding\n",
    "    def embed_tensor(self,inTensor):\n",
    "        return self.seq2seqEncoder.embedding(inTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network object\n",
    "\n",
    "seqToseqNet = Seq2Seq(hidden_unit_dim,layer_count,pr_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss for variable length sequence data\n",
    "# net_output = (batch size, max sequence length, vocab dimension)\n",
    "# target_seqs = (batch size, max sequence length) indices in vocabulary space\n",
    "# target_lengths = list of lengths for each target in batch\n",
    "# loss_criterion = Loss which can be computed on pair of 1d tensors\n",
    "def computeMaskedLoss(net_output,target_seqs,target_lengths,loss_criterion):\n",
    "    \n",
    "    preds = torch.flatten(net_output,start_dim=0,end_dim=1)\n",
    "    targets = target_seqs[:,1:] # Skip sos token\n",
    "    for i in range(len(target_lengths)):\n",
    "        targets[i,(target_lengths[i]-1):] = (-1)*targets.new_ones((1,targets.size(dim=1)-(target_lengths[i]-1)))\n",
    "    \n",
    "    targets = torch.flatten(targets,start_dim=0,end_dim=1)\n",
    "    loss = loss_criterion(preds,targets) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training routine\n",
    "def train_model(net, train_dset, val_dset,ses_lrn = 0.01,ses_tea = 0.5,ses_epochs = 1,ses_batch_size = 16):\n",
    "\n",
    "    train_loader = make_dataloader(train_dset,ses_batch_size)\n",
    "    val_loader = make_dataloader(val_dset,ses_batch_size)\n",
    "    report_interval = rep_int // ses_batch_size\n",
    "    validation_interval = val_int // ses_batch_size\n",
    "    \n",
    "    least_validation_loss = float(\"inf\")\n",
    "    report_interval_counter = 0\n",
    "    validation_interval_counter = 0\n",
    "    val_iter = iter(val_loader)\n",
    "    s = next(val_iter)\n",
    "    \n",
    "    gpu_avail = torch.cuda.is_available()\n",
    "    \n",
    "    if (gpu_avail):\n",
    "        net.cuda()\n",
    "    \n",
    "    loss_criterion = nn.NLLLoss(ignore_index=-1) # Batches padded with -1's\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=ses_lrn)\n",
    "    \n",
    "    for epoch in range(ses_epochs):\n",
    "        \n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, train_data in enumerate(train_loader):\n",
    "            \n",
    "            train_inputs, train_labels = train_data\n",
    "            \n",
    "            if (gpu_avail):\n",
    "                train_inputs = (train_inputs[0].cuda(), train_inputs[1]) \n",
    "                train_labels = (train_labels[0].cuda(), train_labels[1])\n",
    "            \n",
    "            # Zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model outputs\n",
    "            train_outputs = net(train_inputs,train_labels,teacher_forcing_ratio=ses_tea)\n",
    "            \n",
    "            # Compute loss\n",
    "            train_loss = computeMaskedLoss(train_outputs,train_labels[0],train_labels[1],loss_criterion)\n",
    "            \n",
    "            # Compute the loss gradient using the backward method and have the optimizer take a step\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Report status if is time\n",
    "            report_interval_counter += 1\n",
    "            if report_interval_counter >= report_interval:\n",
    "                report_interval_counter = 0\n",
    "                print(f\"{i+1} batches of epoch {epoch+1} completed.  Last Training Loss: {train_loss: .6f}\")\n",
    "            \n",
    "            # Perform validation run if is time\n",
    "            validation_interval_counter += 1\n",
    "            if validation_interval_counter >= validation_interval:\n",
    "                validation_interval_counter = 0\n",
    "                val_loss = 0.0\n",
    "                net.eval()\n",
    "                \n",
    "                # Get validation batch and evaluate model\n",
    "                # Need try block to handle iterator terminating. See\n",
    "                # https://github.com/pytorch/pytorch/issues/1917#issuecomment-433698337\n",
    "                try:\n",
    "                    val_inputs, val_labels = next(val_iter)\n",
    "                except StopIteration:\n",
    "                    val_iter = iter(val_loader)\n",
    "                    val_inputs, val_labels = next(val_iter)\n",
    "                \n",
    "                if (gpu_avail):\n",
    "                    val_inputs = (val_inputs[0].cuda(), val_inputs[1]) \n",
    "                    val_labels = (val_labels[0].cuda(), val_labels[1])\n",
    "                \n",
    "                # Evaluate validation batch outputs against labels\n",
    "                val_outputs = net(val_inputs, val_labels,teacher_forcing_ratio=0)\n",
    "                \n",
    "                # Compute loss\n",
    "                val_loss = computeMaskedLoss(val_outputs,val_labels[0],val_labels[1],loss_criterion)\n",
    "                \n",
    "                # Update min val loss\n",
    "                if val_loss < least_validation_loss:\n",
    "                    least_validation_loss = val_loss\n",
    "                    print(\"Saving model . . .\")\n",
    "                    torch.save(net,\"Min-Validation-Loss-Model-512.pt\")\n",
    "    \n",
    "                # Report\n",
    "                print(f\"Last Validation Loss: {val_loss: .6f}, Lowest Validation Loss: {least_validation_loss: .6f}\")\n",
    "        \n",
    "                # Cleanup after validation\n",
    "                net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  8.109483\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  6.917394\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  7.024382\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  7.071349\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  6.413694\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.718582, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  6.091068\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  5.347987\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  6.402633\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  5.960339\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  5.320562\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  6.176763\n",
      "Last Validation Loss:  8.157598, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  4.570956\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  4.742160\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  6.081700\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  5.931893\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  5.933017\n",
      "Last Validation Loss:  8.373419, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  4.118677\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  4.031346\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  4.059993\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  4.026221\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  5.711045\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  6.007619\n",
      "Last Validation Loss:  8.295064, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  3.614086\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  5.500899\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  5.861258\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  5.655288\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  3.870882\n",
      "Last Validation Loss:  8.712820, Lowest Validation Loss:  7.718582\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  5.606589\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  5.659328\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  5.650393\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  5.773901\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  5.758993\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  3.600132\n",
      "Last Validation Loss:  8.590691, Lowest Validation Loss:  7.718582\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  2.965261\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  5.707190\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  5.598639\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  5.537728\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  6.033118\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  5.447782\n",
      "Last Validation Loss:  8.620613, Lowest Validation Loss:  7.718582\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  2.847313\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  3.017162\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  2.817416\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  5.738846\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  5.769579\n",
      "Last Validation Loss:  8.458989, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  5.497276\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  2.717882\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  5.340139\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  5.545350\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  5.741472\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  2.903450\n",
      "Last Validation Loss:  8.753504, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  2.598603\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  2.635504\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  5.552800\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  2.862314\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  5.742851\n",
      "Last Validation Loss:  8.785384, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  5.663200\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  5.597168\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  2.622277\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  2.625160\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  3.137200\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  2.804962\n",
      "Last Validation Loss:  8.570772, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  5.699440\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  2.690869\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  2.602434\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  2.831387\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  2.576908\n",
      "Last Validation Loss:  8.662434, Lowest Validation Loss:  7.718582\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  5.554807\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  5.471808\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  5.721412\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  2.755113\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  2.598243\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  5.738567\n",
      "Last Validation Loss:  8.373317, Lowest Validation Loss:  7.718582\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  2.178827\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  5.579298\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  5.706399\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  5.729085\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  2.656397\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  5.702926\n",
      "Last Validation Loss:  8.442726, Lowest Validation Loss:  7.718582\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  2.343982\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  2.387504\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  2.298390\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  5.697298\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  2.743001\n",
      "Last Validation Loss:  8.765362, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  2.315581\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  5.485509\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  5.598013\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  5.643146\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  2.338217\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  5.648106\n",
      "Last Validation Loss:  8.465588, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  5.222378\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  2.336646\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  2.577087\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  2.115806\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  5.533278\n",
      "Last Validation Loss:  8.731624, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  2.340560\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  2.242283\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  5.549273\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  5.599278\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  2.250696\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  5.466395\n",
      "Last Validation Loss:  8.502870, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  5.431964\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  5.810216\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  5.527870\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  2.346720\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  5.668534\n",
      "Last Validation Loss:  9.368333, Lowest Validation Loss:  7.718582\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  2.113668\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  5.446705\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  2.253856\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  2.164845\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  5.798206\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  5.569207\n",
      "Last Validation Loss:  8.511001, Lowest Validation Loss:  7.718582\n",
      "4 batches of epoch 21 completed.  Last Training Loss:  5.386855\n",
      "11 batches of epoch 21 completed.  Last Training Loss:  1.918460\n",
      "18 batches of epoch 21 completed.  Last Training Loss:  5.568759\n",
      "25 batches of epoch 21 completed.  Last Training Loss:  2.282303\n",
      "32 batches of epoch 21 completed.  Last Training Loss:  5.304629\n",
      "39 batches of epoch 21 completed.  Last Training Loss:  5.591062\n",
      "Last Validation Loss:  8.695573, Lowest Validation Loss:  7.718582\n",
      "7 batches of epoch 22 completed.  Last Training Loss:  5.312958\n",
      "14 batches of epoch 22 completed.  Last Training Loss:  5.549806\n",
      "21 batches of epoch 22 completed.  Last Training Loss:  2.329256\n",
      "28 batches of epoch 22 completed.  Last Training Loss:  2.010451\n",
      "35 batches of epoch 22 completed.  Last Training Loss:  2.030368\n",
      "Last Validation Loss:  8.871174, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 23 completed.  Last Training Loss:  5.316402\n",
      "10 batches of epoch 23 completed.  Last Training Loss:  5.608445\n",
      "17 batches of epoch 23 completed.  Last Training Loss:  5.512938\n",
      "24 batches of epoch 23 completed.  Last Training Loss:  5.575694\n",
      "31 batches of epoch 23 completed.  Last Training Loss:  5.449731\n",
      "38 batches of epoch 23 completed.  Last Training Loss:  2.019036\n",
      "Last Validation Loss:  9.163216, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 24 completed.  Last Training Loss:  5.270302\n",
      "13 batches of epoch 24 completed.  Last Training Loss:  5.223546\n",
      "20 batches of epoch 24 completed.  Last Training Loss:  5.546712\n",
      "27 batches of epoch 24 completed.  Last Training Loss:  1.997378\n",
      "34 batches of epoch 24 completed.  Last Training Loss:  5.238084\n",
      "Last Validation Loss:  9.586702, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 25 completed.  Last Training Loss:  5.249156\n",
      "9 batches of epoch 25 completed.  Last Training Loss:  2.060539\n",
      "16 batches of epoch 25 completed.  Last Training Loss:  1.749815\n",
      "23 batches of epoch 25 completed.  Last Training Loss:  5.284985\n",
      "30 batches of epoch 25 completed.  Last Training Loss:  2.265094\n",
      "37 batches of epoch 25 completed.  Last Training Loss:  5.640395\n",
      "Last Validation Loss:  9.566199, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 26 completed.  Last Training Loss:  1.920389\n",
      "12 batches of epoch 26 completed.  Last Training Loss:  5.648558\n",
      "19 batches of epoch 26 completed.  Last Training Loss:  2.164156\n",
      "26 batches of epoch 26 completed.  Last Training Loss:  1.920487\n",
      "33 batches of epoch 26 completed.  Last Training Loss:  5.604807\n",
      "Last Validation Loss:  8.910740, Lowest Validation Loss:  7.718582\n",
      "1 batches of epoch 27 completed.  Last Training Loss:  5.004691\n",
      "8 batches of epoch 27 completed.  Last Training Loss:  5.142301\n",
      "15 batches of epoch 27 completed.  Last Training Loss:  5.286826\n",
      "22 batches of epoch 27 completed.  Last Training Loss:  5.130868\n",
      "29 batches of epoch 27 completed.  Last Training Loss:  1.980657\n",
      "36 batches of epoch 27 completed.  Last Training Loss:  5.449553\n",
      "Last Validation Loss:  8.851751, Lowest Validation Loss:  7.718582\n",
      "4 batches of epoch 28 completed.  Last Training Loss:  5.080803\n",
      "11 batches of epoch 28 completed.  Last Training Loss:  1.812688\n",
      "18 batches of epoch 28 completed.  Last Training Loss:  1.958280\n",
      "25 batches of epoch 28 completed.  Last Training Loss:  2.027733\n",
      "32 batches of epoch 28 completed.  Last Training Loss:  5.376668\n",
      "39 batches of epoch 28 completed.  Last Training Loss:  2.017956\n",
      "Last Validation Loss:  9.523580, Lowest Validation Loss:  7.718582\n",
      "7 batches of epoch 29 completed.  Last Training Loss:  1.907330\n",
      "14 batches of epoch 29 completed.  Last Training Loss:  5.176628\n",
      "21 batches of epoch 29 completed.  Last Training Loss:  5.356093\n",
      "28 batches of epoch 29 completed.  Last Training Loss:  2.157924\n",
      "35 batches of epoch 29 completed.  Last Training Loss:  5.124366\n",
      "Last Validation Loss:  9.188167, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 30 completed.  Last Training Loss:  1.956112\n",
      "10 batches of epoch 30 completed.  Last Training Loss:  2.006650\n",
      "17 batches of epoch 30 completed.  Last Training Loss:  5.020967\n",
      "24 batches of epoch 30 completed.  Last Training Loss:  5.350574\n",
      "31 batches of epoch 30 completed.  Last Training Loss:  5.215660\n",
      "38 batches of epoch 30 completed.  Last Training Loss:  2.119072\n",
      "Last Validation Loss:  9.291683, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 31 completed.  Last Training Loss:  1.942591\n",
      "13 batches of epoch 31 completed.  Last Training Loss:  5.462439\n",
      "20 batches of epoch 31 completed.  Last Training Loss:  2.005028\n",
      "27 batches of epoch 31 completed.  Last Training Loss:  5.289596\n",
      "34 batches of epoch 31 completed.  Last Training Loss:  5.316785\n",
      "Last Validation Loss:  9.425632, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 32 completed.  Last Training Loss:  5.000818\n",
      "9 batches of epoch 32 completed.  Last Training Loss:  2.030211\n",
      "16 batches of epoch 32 completed.  Last Training Loss:  2.144779\n",
      "23 batches of epoch 32 completed.  Last Training Loss:  4.971336\n",
      "30 batches of epoch 32 completed.  Last Training Loss:  5.311616\n",
      "37 batches of epoch 32 completed.  Last Training Loss:  5.232960\n",
      "Last Validation Loss:  9.061449, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 33 completed.  Last Training Loss:  4.989417\n",
      "12 batches of epoch 33 completed.  Last Training Loss:  1.956882\n",
      "19 batches of epoch 33 completed.  Last Training Loss:  2.052024\n",
      "26 batches of epoch 33 completed.  Last Training Loss:  5.253683\n",
      "33 batches of epoch 33 completed.  Last Training Loss:  2.170699\n",
      "Last Validation Loss:  9.378868, Lowest Validation Loss:  7.718582\n",
      "1 batches of epoch 34 completed.  Last Training Loss:  5.080402\n",
      "8 batches of epoch 34 completed.  Last Training Loss:  1.985345\n",
      "15 batches of epoch 34 completed.  Last Training Loss:  5.113409\n",
      "22 batches of epoch 34 completed.  Last Training Loss:  1.960013\n",
      "29 batches of epoch 34 completed.  Last Training Loss:  5.488974\n",
      "36 batches of epoch 34 completed.  Last Training Loss:  5.417940\n",
      "Last Validation Loss:  9.016054, Lowest Validation Loss:  7.718582\n",
      "4 batches of epoch 35 completed.  Last Training Loss:  1.873708\n",
      "11 batches of epoch 35 completed.  Last Training Loss:  4.621179\n",
      "18 batches of epoch 35 completed.  Last Training Loss:  5.063254\n",
      "25 batches of epoch 35 completed.  Last Training Loss:  1.838583\n",
      "32 batches of epoch 35 completed.  Last Training Loss:  5.154242\n",
      "39 batches of epoch 35 completed.  Last Training Loss:  2.046791\n",
      "Last Validation Loss:  10.034203, Lowest Validation Loss:  7.718582\n",
      "7 batches of epoch 36 completed.  Last Training Loss:  5.061582\n",
      "14 batches of epoch 36 completed.  Last Training Loss:  2.050248\n",
      "21 batches of epoch 36 completed.  Last Training Loss:  2.030113\n",
      "28 batches of epoch 36 completed.  Last Training Loss:  5.062455\n",
      "35 batches of epoch 36 completed.  Last Training Loss:  1.777152\n",
      "Last Validation Loss:  9.533905, Lowest Validation Loss:  7.718582\n",
      "3 batches of epoch 37 completed.  Last Training Loss:  1.909781\n",
      "10 batches of epoch 37 completed.  Last Training Loss:  1.668445\n",
      "17 batches of epoch 37 completed.  Last Training Loss:  1.906255\n",
      "24 batches of epoch 37 completed.  Last Training Loss:  5.163049\n",
      "31 batches of epoch 37 completed.  Last Training Loss:  1.889074\n",
      "38 batches of epoch 37 completed.  Last Training Loss:  5.358519\n",
      "Last Validation Loss:  9.719220, Lowest Validation Loss:  7.718582\n",
      "6 batches of epoch 38 completed.  Last Training Loss:  4.847289\n",
      "13 batches of epoch 38 completed.  Last Training Loss:  5.021298\n",
      "20 batches of epoch 38 completed.  Last Training Loss:  4.751302\n",
      "27 batches of epoch 38 completed.  Last Training Loss:  2.002374\n",
      "34 batches of epoch 38 completed.  Last Training Loss:  1.896784\n",
      "Last Validation Loss:  9.297637, Lowest Validation Loss:  7.718582\n",
      "2 batches of epoch 39 completed.  Last Training Loss:  4.493650\n",
      "9 batches of epoch 39 completed.  Last Training Loss:  4.841282\n",
      "16 batches of epoch 39 completed.  Last Training Loss:  5.003374\n",
      "23 batches of epoch 39 completed.  Last Training Loss:  4.990809\n",
      "30 batches of epoch 39 completed.  Last Training Loss:  4.742260\n",
      "37 batches of epoch 39 completed.  Last Training Loss:  4.988492\n",
      "Last Validation Loss:  10.474218, Lowest Validation Loss:  7.718582\n",
      "5 batches of epoch 40 completed.  Last Training Loss:  4.716184\n",
      "12 batches of epoch 40 completed.  Last Training Loss:  1.661107\n",
      "19 batches of epoch 40 completed.  Last Training Loss:  1.884964\n",
      "26 batches of epoch 40 completed.  Last Training Loss:  5.190221\n",
      "33 batches of epoch 40 completed.  Last Training Loss:  4.892367\n",
      "Last Validation Loss:  9.984978, Lowest Validation Loss:  7.718582\n"
     ]
    }
   ],
   "source": [
    "# Perform first training run\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.01, ses_tea = 0.5, ses_epochs = 40, ses_batch_size = 128)\n",
    "torch.save(seqToseqNet,'afterRunOne.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  1.808320\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  4.769550\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  1.495469\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  4.609407\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  1.767477\n",
      "Saving model . . .\n",
      "Last Validation Loss:  9.578876, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  1.358416\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  1.490441\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  4.237613\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  1.578929\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  1.438875\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  1.537268\n",
      "Last Validation Loss:  9.810240, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  4.073042\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  1.257350\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  1.342730\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  1.354382\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  4.571116\n",
      "Last Validation Loss:  10.293552, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  1.189668\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  1.380434\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  4.019353\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  3.843496\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  1.253560\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  4.171446\n",
      "Last Validation Loss:  10.550433, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  3.927382\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  3.728670\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  4.640764\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  1.228266\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  1.289832\n",
      "Last Validation Loss:  10.211733, Lowest Validation Loss:  9.578876\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  3.335825\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  3.749165\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  3.816450\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  4.566025\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  1.406502\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  1.254904\n",
      "Last Validation Loss:  10.173541, Lowest Validation Loss:  9.578876\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  1.162466\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  3.842895\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  1.261299\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  1.208596\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  1.260779\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  1.213320\n",
      "Last Validation Loss:  10.731009, Lowest Validation Loss:  9.578876\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  1.028530\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  1.107578\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  1.154668\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  1.171971\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  1.301277\n",
      "Last Validation Loss:  10.666454, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  3.038822\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  3.122536\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  3.358673\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  1.004752\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  1.348856\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  4.172021\n",
      "Last Validation Loss:  10.981624, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  2.696984\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  1.102516\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  2.938928\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  1.101811\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  3.100396\n",
      "Last Validation Loss:  10.474461, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  3.106153\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  0.957719\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  0.994279\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  1.070720\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  1.137930\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  1.066493\n",
      "Last Validation Loss:  10.819942, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  0.851185\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  2.599494\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  3.263393\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  3.627286\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  1.021870\n",
      "Last Validation Loss:  10.751974, Lowest Validation Loss:  9.578876\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  2.716046\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  2.481135\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  0.986002\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  0.983038\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  2.931940\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  0.912681\n",
      "Last Validation Loss:  10.985425, Lowest Validation Loss:  9.578876\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  0.856691\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  1.915371\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  2.809879\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  2.697335\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  3.373152\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  2.937225\n",
      "Last Validation Loss:  10.954948, Lowest Validation Loss:  9.578876\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  2.753702\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  2.547666\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  2.366052\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  0.957430\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  0.704024\n",
      "Last Validation Loss:  11.297980, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  2.032089\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  0.725342\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  1.903045\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  2.226876\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  2.012817\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  2.805748\n",
      "Last Validation Loss:  11.283713, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  1.937706\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  0.742324\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  0.867752\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  0.754142\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  0.850717\n",
      "Last Validation Loss:  11.311930, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  1.543494\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  0.666328\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  1.654027\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  2.227495\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  0.734325\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  2.430721\n",
      "Last Validation Loss:  12.154165, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  0.574726\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  1.844222\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  0.626490\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  2.134980\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  1.847549\n",
      "Last Validation Loss:  10.860792, Lowest Validation Loss:  9.578876\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  0.631983\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  0.691060\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  0.674840\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  0.720375\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  1.298008\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  1.623421\n",
      "Last Validation Loss:  11.112949, Lowest Validation Loss:  9.578876\n",
      "4 batches of epoch 21 completed.  Last Training Loss:  0.605380\n",
      "11 batches of epoch 21 completed.  Last Training Loss:  0.593632\n",
      "18 batches of epoch 21 completed.  Last Training Loss:  1.553357\n",
      "25 batches of epoch 21 completed.  Last Training Loss:  1.377402\n",
      "32 batches of epoch 21 completed.  Last Training Loss:  1.743825\n",
      "39 batches of epoch 21 completed.  Last Training Loss:  1.621544\n",
      "Last Validation Loss:  11.748354, Lowest Validation Loss:  9.578876\n",
      "7 batches of epoch 22 completed.  Last Training Loss:  1.076859\n",
      "14 batches of epoch 22 completed.  Last Training Loss:  0.491076\n",
      "21 batches of epoch 22 completed.  Last Training Loss:  0.483844\n",
      "28 batches of epoch 22 completed.  Last Training Loss:  1.753834\n",
      "35 batches of epoch 22 completed.  Last Training Loss:  1.242684\n",
      "Last Validation Loss:  11.163987, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 23 completed.  Last Training Loss:  0.499687\n",
      "10 batches of epoch 23 completed.  Last Training Loss:  1.360131\n",
      "17 batches of epoch 23 completed.  Last Training Loss:  0.458680\n",
      "24 batches of epoch 23 completed.  Last Training Loss:  1.508603\n",
      "31 batches of epoch 23 completed.  Last Training Loss:  0.567139\n",
      "38 batches of epoch 23 completed.  Last Training Loss:  1.896107\n",
      "Last Validation Loss:  11.394968, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 24 completed.  Last Training Loss:  0.466505\n",
      "13 batches of epoch 24 completed.  Last Training Loss:  1.322258\n",
      "20 batches of epoch 24 completed.  Last Training Loss:  0.476054\n",
      "27 batches of epoch 24 completed.  Last Training Loss:  1.203076\n",
      "34 batches of epoch 24 completed.  Last Training Loss:  0.507293\n",
      "Last Validation Loss:  11.620356, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 25 completed.  Last Training Loss:  0.406130\n",
      "9 batches of epoch 25 completed.  Last Training Loss:  0.359569\n",
      "16 batches of epoch 25 completed.  Last Training Loss:  0.984882\n",
      "23 batches of epoch 25 completed.  Last Training Loss:  1.063886\n",
      "30 batches of epoch 25 completed.  Last Training Loss:  1.324737\n",
      "37 batches of epoch 25 completed.  Last Training Loss:  1.361365\n",
      "Last Validation Loss:  11.370383, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 26 completed.  Last Training Loss:  0.318730\n",
      "12 batches of epoch 26 completed.  Last Training Loss:  0.386510\n",
      "19 batches of epoch 26 completed.  Last Training Loss:  0.692408\n",
      "26 batches of epoch 26 completed.  Last Training Loss:  1.123437\n",
      "33 batches of epoch 26 completed.  Last Training Loss:  1.204861\n",
      "Last Validation Loss:  11.778173, Lowest Validation Loss:  9.578876\n",
      "1 batches of epoch 27 completed.  Last Training Loss:  0.312783\n",
      "8 batches of epoch 27 completed.  Last Training Loss:  0.581498\n",
      "15 batches of epoch 27 completed.  Last Training Loss:  1.132680\n",
      "22 batches of epoch 27 completed.  Last Training Loss:  0.444529\n",
      "29 batches of epoch 27 completed.  Last Training Loss:  0.417848\n",
      "36 batches of epoch 27 completed.  Last Training Loss:  0.464048\n",
      "Last Validation Loss:  12.009868, Lowest Validation Loss:  9.578876\n",
      "4 batches of epoch 28 completed.  Last Training Loss:  0.326508\n",
      "11 batches of epoch 28 completed.  Last Training Loss:  0.572202\n",
      "18 batches of epoch 28 completed.  Last Training Loss:  0.775614\n",
      "25 batches of epoch 28 completed.  Last Training Loss:  0.420978\n",
      "32 batches of epoch 28 completed.  Last Training Loss:  0.739867\n",
      "39 batches of epoch 28 completed.  Last Training Loss:  0.826040\n",
      "Last Validation Loss:  11.656219, Lowest Validation Loss:  9.578876\n",
      "7 batches of epoch 29 completed.  Last Training Loss:  0.310264\n",
      "14 batches of epoch 29 completed.  Last Training Loss:  0.916485\n",
      "21 batches of epoch 29 completed.  Last Training Loss:  0.437095\n",
      "28 batches of epoch 29 completed.  Last Training Loss:  0.369178\n",
      "35 batches of epoch 29 completed.  Last Training Loss:  0.846625\n",
      "Last Validation Loss:  11.659569, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 30 completed.  Last Training Loss:  0.240827\n",
      "10 batches of epoch 30 completed.  Last Training Loss:  1.271337\n",
      "17 batches of epoch 30 completed.  Last Training Loss:  0.742467\n",
      "24 batches of epoch 30 completed.  Last Training Loss:  0.277527\n",
      "31 batches of epoch 30 completed.  Last Training Loss:  0.415724\n",
      "38 batches of epoch 30 completed.  Last Training Loss:  1.179747\n",
      "Last Validation Loss:  12.321413, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 31 completed.  Last Training Loss:  0.734637\n",
      "13 batches of epoch 31 completed.  Last Training Loss:  0.444522\n",
      "20 batches of epoch 31 completed.  Last Training Loss:  1.012320\n",
      "27 batches of epoch 31 completed.  Last Training Loss:  0.276685\n",
      "34 batches of epoch 31 completed.  Last Training Loss:  0.342409\n",
      "Last Validation Loss:  11.932951, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 32 completed.  Last Training Loss:  0.539263\n",
      "9 batches of epoch 32 completed.  Last Training Loss:  0.358236\n",
      "16 batches of epoch 32 completed.  Last Training Loss:  0.262709\n",
      "23 batches of epoch 32 completed.  Last Training Loss:  0.869522\n",
      "30 batches of epoch 32 completed.  Last Training Loss:  0.393846\n",
      "37 batches of epoch 32 completed.  Last Training Loss:  0.487954\n",
      "Last Validation Loss:  12.060409, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 33 completed.  Last Training Loss:  0.649960\n",
      "12 batches of epoch 33 completed.  Last Training Loss:  0.233009\n",
      "19 batches of epoch 33 completed.  Last Training Loss:  0.245458\n",
      "26 batches of epoch 33 completed.  Last Training Loss:  0.870031\n",
      "33 batches of epoch 33 completed.  Last Training Loss:  0.235740\n",
      "Last Validation Loss:  12.175100, Lowest Validation Loss:  9.578876\n",
      "1 batches of epoch 34 completed.  Last Training Loss:  0.244329\n",
      "8 batches of epoch 34 completed.  Last Training Loss:  0.212376\n",
      "15 batches of epoch 34 completed.  Last Training Loss:  0.202122\n",
      "22 batches of epoch 34 completed.  Last Training Loss:  0.278395\n",
      "29 batches of epoch 34 completed.  Last Training Loss:  0.259243\n",
      "36 batches of epoch 34 completed.  Last Training Loss:  0.247193\n",
      "Last Validation Loss:  12.157186, Lowest Validation Loss:  9.578876\n",
      "4 batches of epoch 35 completed.  Last Training Loss:  0.490744\n",
      "11 batches of epoch 35 completed.  Last Training Loss:  0.381588\n",
      "18 batches of epoch 35 completed.  Last Training Loss:  0.476349\n",
      "25 batches of epoch 35 completed.  Last Training Loss:  0.247370\n",
      "32 batches of epoch 35 completed.  Last Training Loss:  0.771245\n",
      "39 batches of epoch 35 completed.  Last Training Loss:  0.292309\n",
      "Last Validation Loss:  12.645297, Lowest Validation Loss:  9.578876\n",
      "7 batches of epoch 36 completed.  Last Training Loss:  0.220823\n",
      "14 batches of epoch 36 completed.  Last Training Loss:  0.579333\n",
      "21 batches of epoch 36 completed.  Last Training Loss:  0.190754\n",
      "28 batches of epoch 36 completed.  Last Training Loss:  0.446895\n",
      "35 batches of epoch 36 completed.  Last Training Loss:  0.330123\n",
      "Last Validation Loss:  12.590166, Lowest Validation Loss:  9.578876\n",
      "3 batches of epoch 37 completed.  Last Training Loss:  0.479711\n",
      "10 batches of epoch 37 completed.  Last Training Loss:  0.460920\n",
      "17 batches of epoch 37 completed.  Last Training Loss:  0.375905\n",
      "24 batches of epoch 37 completed.  Last Training Loss:  0.217194\n",
      "31 batches of epoch 37 completed.  Last Training Loss:  0.415295\n",
      "38 batches of epoch 37 completed.  Last Training Loss:  0.624764\n",
      "Last Validation Loss:  12.568646, Lowest Validation Loss:  9.578876\n",
      "6 batches of epoch 38 completed.  Last Training Loss:  0.288549\n",
      "13 batches of epoch 38 completed.  Last Training Loss:  0.329975\n",
      "20 batches of epoch 38 completed.  Last Training Loss:  0.278373\n",
      "27 batches of epoch 38 completed.  Last Training Loss:  0.259096\n",
      "34 batches of epoch 38 completed.  Last Training Loss:  0.551395\n",
      "Last Validation Loss:  13.145230, Lowest Validation Loss:  9.578876\n",
      "2 batches of epoch 39 completed.  Last Training Loss:  0.367914\n",
      "9 batches of epoch 39 completed.  Last Training Loss:  0.282443\n",
      "16 batches of epoch 39 completed.  Last Training Loss:  0.577636\n",
      "23 batches of epoch 39 completed.  Last Training Loss:  0.221244\n",
      "30 batches of epoch 39 completed.  Last Training Loss:  0.144799\n",
      "37 batches of epoch 39 completed.  Last Training Loss:  0.406865\n",
      "Last Validation Loss:  12.063094, Lowest Validation Loss:  9.578876\n",
      "5 batches of epoch 40 completed.  Last Training Loss:  0.120602\n",
      "12 batches of epoch 40 completed.  Last Training Loss:  0.178204\n",
      "19 batches of epoch 40 completed.  Last Training Loss:  0.331824\n",
      "26 batches of epoch 40 completed.  Last Training Loss:  0.181806\n",
      "33 batches of epoch 40 completed.  Last Training Loss:  0.124497\n",
      "Last Validation Loss:  12.481177, Lowest Validation Loss:  9.578876\n"
     ]
    }
   ],
   "source": [
    "# Continue training lower learning rate to 0.005\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.005, ses_tea = 0.5, ses_epochs = 40, ses_batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  0.447459\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  0.679614\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  0.504116\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  0.636513\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  0.665016\n",
      "Saving model . . .\n",
      "Last Validation Loss:  11.611863, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  0.431535\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  0.524961\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  0.180326\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  0.552641\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  0.617662\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  0.667711\n",
      "Last Validation Loss:  12.049320, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  0.223446\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  0.269266\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  0.365927\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  0.306165\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  0.820527\n",
      "Last Validation Loss:  11.692115, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  0.191602\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  0.198220\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  0.222259\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  0.428989\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  0.323149\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  0.381508\n",
      "Last Validation Loss:  11.628085, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  0.233415\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  0.198217\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  0.200124\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  0.186887\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  0.172891\n",
      "Last Validation Loss:  12.145521, Lowest Validation Loss:  11.611863\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  0.272524\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  0.376784\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  0.211353\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  0.203748\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  0.115965\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  0.505920\n",
      "Last Validation Loss:  12.669468, Lowest Validation Loss:  11.611863\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  0.197080\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  0.170335\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  0.196503\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  0.274221\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  0.196648\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  0.242532\n",
      "Last Validation Loss:  12.374950, Lowest Validation Loss:  11.611863\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  0.340512\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  0.607620\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  0.129111\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  0.214940\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  0.333154\n",
      "Last Validation Loss:  12.209797, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  0.152403\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  0.187838\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  0.137741\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  0.515132\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  0.291078\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  0.228960\n",
      "Last Validation Loss:  13.165530, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  0.208245\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  0.400236\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  0.133403\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  0.123314\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  0.119925\n",
      "Last Validation Loss:  11.841306, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  0.089766\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  0.133373\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  0.285934\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  0.361709\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  0.257842\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  0.244558\n",
      "Last Validation Loss:  12.024076, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  0.105560\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  0.230141\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  0.122475\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  0.215037\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  0.326087\n",
      "Last Validation Loss:  12.162036, Lowest Validation Loss:  11.611863\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  0.120788\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  0.084088\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  0.251662\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  0.084917\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  0.274834\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  0.390830\n",
      "Last Validation Loss:  12.475586, Lowest Validation Loss:  11.611863\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  0.074387\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  0.120955\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  0.152474\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  0.201789\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  0.181914\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  0.260553\n",
      "Last Validation Loss:  12.344266, Lowest Validation Loss:  11.611863\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  0.218377\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  0.235049\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  0.186744\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  0.141469\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  0.173585\n",
      "Last Validation Loss:  12.243918, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  0.115372\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  0.532674\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  0.070830\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  0.111757\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  0.107192\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  0.173606\n",
      "Last Validation Loss:  12.911282, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  0.067125\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  0.265277\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  0.303796\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  0.196832\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  0.435131\n",
      "Last Validation Loss:  13.100999, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  0.616095\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  0.077531\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  0.044832\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  0.075942\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  0.090132\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  0.184809\n",
      "Last Validation Loss:  12.683380, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  0.354954\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  0.346581\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  0.141075\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  0.160729\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  0.049630\n",
      "Last Validation Loss:  13.447079, Lowest Validation Loss:  11.611863\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  0.067651\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  0.047470\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  0.040965\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  0.178895\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  0.088726\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  0.113880\n",
      "Last Validation Loss:  12.894161, Lowest Validation Loss:  11.611863\n",
      "4 batches of epoch 21 completed.  Last Training Loss:  0.288593\n",
      "11 batches of epoch 21 completed.  Last Training Loss:  0.077420\n",
      "18 batches of epoch 21 completed.  Last Training Loss:  0.054788\n",
      "25 batches of epoch 21 completed.  Last Training Loss:  0.072940\n",
      "32 batches of epoch 21 completed.  Last Training Loss:  0.044605\n",
      "39 batches of epoch 21 completed.  Last Training Loss:  0.051680\n",
      "Last Validation Loss:  13.206158, Lowest Validation Loss:  11.611863\n",
      "7 batches of epoch 22 completed.  Last Training Loss:  0.209460\n",
      "14 batches of epoch 22 completed.  Last Training Loss:  0.082941\n",
      "21 batches of epoch 22 completed.  Last Training Loss:  0.058575\n",
      "28 batches of epoch 22 completed.  Last Training Loss:  0.079831\n",
      "35 batches of epoch 22 completed.  Last Training Loss:  0.090314\n",
      "Last Validation Loss:  13.098071, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 23 completed.  Last Training Loss:  0.093647\n",
      "10 batches of epoch 23 completed.  Last Training Loss:  0.037280\n",
      "17 batches of epoch 23 completed.  Last Training Loss:  0.071233\n",
      "24 batches of epoch 23 completed.  Last Training Loss:  0.073687\n",
      "31 batches of epoch 23 completed.  Last Training Loss:  0.062202\n",
      "38 batches of epoch 23 completed.  Last Training Loss:  0.468658\n",
      "Last Validation Loss:  12.943842, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 24 completed.  Last Training Loss:  0.064221\n",
      "13 batches of epoch 24 completed.  Last Training Loss:  0.056091\n",
      "20 batches of epoch 24 completed.  Last Training Loss:  0.024046\n",
      "27 batches of epoch 24 completed.  Last Training Loss:  0.296050\n",
      "34 batches of epoch 24 completed.  Last Training Loss:  0.107844\n",
      "Last Validation Loss:  12.510317, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 25 completed.  Last Training Loss:  0.089137\n",
      "9 batches of epoch 25 completed.  Last Training Loss:  0.102863\n",
      "16 batches of epoch 25 completed.  Last Training Loss:  0.055006\n",
      "23 batches of epoch 25 completed.  Last Training Loss:  0.070187\n",
      "30 batches of epoch 25 completed.  Last Training Loss:  0.057267\n",
      "37 batches of epoch 25 completed.  Last Training Loss:  0.028816\n",
      "Last Validation Loss:  13.012803, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 26 completed.  Last Training Loss:  0.152738\n",
      "12 batches of epoch 26 completed.  Last Training Loss:  0.378702\n",
      "19 batches of epoch 26 completed.  Last Training Loss:  0.074312\n",
      "26 batches of epoch 26 completed.  Last Training Loss:  0.039208\n",
      "33 batches of epoch 26 completed.  Last Training Loss:  0.037398\n",
      "Last Validation Loss:  13.707434, Lowest Validation Loss:  11.611863\n",
      "1 batches of epoch 27 completed.  Last Training Loss:  0.082700\n",
      "8 batches of epoch 27 completed.  Last Training Loss:  0.041641\n",
      "15 batches of epoch 27 completed.  Last Training Loss:  0.043668\n",
      "22 batches of epoch 27 completed.  Last Training Loss:  0.228054\n",
      "29 batches of epoch 27 completed.  Last Training Loss:  0.099162\n",
      "36 batches of epoch 27 completed.  Last Training Loss:  0.038729\n",
      "Last Validation Loss:  13.330820, Lowest Validation Loss:  11.611863\n",
      "4 batches of epoch 28 completed.  Last Training Loss:  0.136253\n",
      "11 batches of epoch 28 completed.  Last Training Loss:  0.062248\n",
      "18 batches of epoch 28 completed.  Last Training Loss:  0.320766\n",
      "25 batches of epoch 28 completed.  Last Training Loss:  0.047423\n",
      "32 batches of epoch 28 completed.  Last Training Loss:  0.029761\n",
      "39 batches of epoch 28 completed.  Last Training Loss:  0.049192\n",
      "Last Validation Loss:  12.829309, Lowest Validation Loss:  11.611863\n",
      "7 batches of epoch 29 completed.  Last Training Loss:  0.188010\n",
      "14 batches of epoch 29 completed.  Last Training Loss:  0.150011\n",
      "21 batches of epoch 29 completed.  Last Training Loss:  0.033430\n",
      "28 batches of epoch 29 completed.  Last Training Loss:  0.023970\n",
      "35 batches of epoch 29 completed.  Last Training Loss:  0.034228\n",
      "Last Validation Loss:  13.086581, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 30 completed.  Last Training Loss:  0.031333\n",
      "10 batches of epoch 30 completed.  Last Training Loss:  0.020955\n",
      "17 batches of epoch 30 completed.  Last Training Loss:  0.072645\n",
      "24 batches of epoch 30 completed.  Last Training Loss:  0.057568\n",
      "31 batches of epoch 30 completed.  Last Training Loss:  0.026926\n",
      "38 batches of epoch 30 completed.  Last Training Loss:  0.120546\n",
      "Last Validation Loss:  13.125271, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 31 completed.  Last Training Loss:  0.025714\n",
      "13 batches of epoch 31 completed.  Last Training Loss:  0.139844\n",
      "20 batches of epoch 31 completed.  Last Training Loss:  0.090056\n",
      "27 batches of epoch 31 completed.  Last Training Loss:  0.025094\n",
      "34 batches of epoch 31 completed.  Last Training Loss:  0.187844\n",
      "Last Validation Loss:  13.288262, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 32 completed.  Last Training Loss:  0.021231\n",
      "9 batches of epoch 32 completed.  Last Training Loss:  0.194112\n",
      "16 batches of epoch 32 completed.  Last Training Loss:  0.101521\n",
      "23 batches of epoch 32 completed.  Last Training Loss:  0.045989\n",
      "30 batches of epoch 32 completed.  Last Training Loss:  0.181443\n",
      "37 batches of epoch 32 completed.  Last Training Loss:  0.016471\n",
      "Last Validation Loss:  13.090050, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 33 completed.  Last Training Loss:  0.047506\n",
      "12 batches of epoch 33 completed.  Last Training Loss:  0.053371\n",
      "19 batches of epoch 33 completed.  Last Training Loss:  0.017910\n",
      "26 batches of epoch 33 completed.  Last Training Loss:  0.017265\n",
      "33 batches of epoch 33 completed.  Last Training Loss:  0.038067\n",
      "Last Validation Loss:  13.398564, Lowest Validation Loss:  11.611863\n",
      "1 batches of epoch 34 completed.  Last Training Loss:  0.017793\n",
      "8 batches of epoch 34 completed.  Last Training Loss:  0.215879\n",
      "15 batches of epoch 34 completed.  Last Training Loss:  0.091282\n",
      "22 batches of epoch 34 completed.  Last Training Loss:  0.038916\n",
      "29 batches of epoch 34 completed.  Last Training Loss:  0.028268\n",
      "36 batches of epoch 34 completed.  Last Training Loss:  0.026013\n",
      "Last Validation Loss:  13.015349, Lowest Validation Loss:  11.611863\n",
      "4 batches of epoch 35 completed.  Last Training Loss:  0.016175\n",
      "11 batches of epoch 35 completed.  Last Training Loss:  0.067570\n",
      "18 batches of epoch 35 completed.  Last Training Loss:  0.022088\n",
      "25 batches of epoch 35 completed.  Last Training Loss:  0.017438\n",
      "32 batches of epoch 35 completed.  Last Training Loss:  0.027068\n",
      "39 batches of epoch 35 completed.  Last Training Loss:  0.153470\n",
      "Last Validation Loss:  13.996500, Lowest Validation Loss:  11.611863\n",
      "7 batches of epoch 36 completed.  Last Training Loss:  0.019776\n",
      "14 batches of epoch 36 completed.  Last Training Loss:  0.022128\n",
      "21 batches of epoch 36 completed.  Last Training Loss:  0.090477\n",
      "28 batches of epoch 36 completed.  Last Training Loss:  0.096416\n",
      "35 batches of epoch 36 completed.  Last Training Loss:  0.183048\n",
      "Last Validation Loss:  13.539925, Lowest Validation Loss:  11.611863\n",
      "3 batches of epoch 37 completed.  Last Training Loss:  0.020010\n",
      "10 batches of epoch 37 completed.  Last Training Loss:  0.026103\n",
      "17 batches of epoch 37 completed.  Last Training Loss:  0.072397\n",
      "24 batches of epoch 37 completed.  Last Training Loss:  0.142561\n",
      "31 batches of epoch 37 completed.  Last Training Loss:  0.022147\n",
      "38 batches of epoch 37 completed.  Last Training Loss:  0.013444\n",
      "Last Validation Loss:  13.641244, Lowest Validation Loss:  11.611863\n",
      "6 batches of epoch 38 completed.  Last Training Loss:  0.026737\n",
      "13 batches of epoch 38 completed.  Last Training Loss:  0.032583\n",
      "20 batches of epoch 38 completed.  Last Training Loss:  0.013853\n",
      "27 batches of epoch 38 completed.  Last Training Loss:  0.067745\n",
      "34 batches of epoch 38 completed.  Last Training Loss:  0.017637\n",
      "Last Validation Loss:  13.282810, Lowest Validation Loss:  11.611863\n",
      "2 batches of epoch 39 completed.  Last Training Loss:  0.042234\n",
      "9 batches of epoch 39 completed.  Last Training Loss:  0.030411\n",
      "16 batches of epoch 39 completed.  Last Training Loss:  0.026807\n",
      "23 batches of epoch 39 completed.  Last Training Loss:  0.016186\n",
      "30 batches of epoch 39 completed.  Last Training Loss:  0.018689\n",
      "37 batches of epoch 39 completed.  Last Training Loss:  0.021663\n",
      "Last Validation Loss:  13.550744, Lowest Validation Loss:  11.611863\n",
      "5 batches of epoch 40 completed.  Last Training Loss:  0.012129\n",
      "12 batches of epoch 40 completed.  Last Training Loss:  0.201463\n",
      "19 batches of epoch 40 completed.  Last Training Loss:  0.023957\n",
      "26 batches of epoch 40 completed.  Last Training Loss:  0.062550\n",
      "33 batches of epoch 40 completed.  Last Training Loss:  0.017656\n",
      "Last Validation Loss:  13.705342, Lowest Validation Loss:  11.611863\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunTwo.pt')\n",
    "\n",
    "# See if can make training loss less than one without any teaching\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.005, ses_tea = 0.0, ses_epochs = 40, ses_batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return inference given model, keyvectors vocabulary,prompt, maximum response length\n",
    "def return_inference(model,emb_kv,prompt,maxLen):\n",
    "    \n",
    "    # Bounded prompt\n",
    "    bounded_prompt = prompt + ' ' + eosToken\n",
    "    \n",
    "    # Convert prompt to embedding vectors\n",
    "    prompt_model_input = torch.tensor(tokensToIndices(prepare_text(bounded_prompt,emb_kv),emb_kv), dtype=torch.int64)\n",
    "    prompt_model_input = prompt_model_input.unsqueeze(dim=0)\n",
    "    prompt_question_lengths = torch.tensor([prompt_model_input.size(dim=1)], dtype=torch.int64)\n",
    "    \n",
    "    # Prepare unused target for forward method, except to feed in soseq token at start\n",
    "    unused_target = emb_kv.get_index(sosToken)*torch.ones(maxLen,dtype=torch.int64).unsqueeze(dim=0)\n",
    "    unused_lengths = torch.tensor([maxLen],dtype=torch.int64)\n",
    "        \n",
    "    # Get output tensor\n",
    "    output = model.to(\"cpu\")((prompt_model_input,prompt_question_lengths), \\\n",
    "                                     (unused_target,unused_lengths), \\\n",
    "                                     teacher_forcing_ratio=0)\n",
    "    outString = \"\"\n",
    "    \n",
    "    # Convert embeddings to words\n",
    "    for i in range(output.size(dim=1)):\n",
    "        next_word = emb_kv.index_to_key[torch.argmax(output[0][i])]\n",
    "        if next_word == eosToken:\n",
    "            outString += \" \" + next_word\n",
    "            break\n",
    "        else:\n",
    "            outString += \" \" + next_word\n",
    "    \n",
    "    return outString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " $ million eoseq\n",
      " polish eoseq\n",
      " september eoseq\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_one = \"What was the size of the notre dame endowment when theodore hesburgh became president?\"\n",
    "prompt_two = \"Who won the most music awards?\"\n",
    "prompt_three = \"Where was the last war?\"\n",
    "\n",
    "for prompt in [prompt_one, prompt_two, prompt_three]:\n",
    "    print(return_inference(seqToseqNet,pr_kv,prompt,40))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
