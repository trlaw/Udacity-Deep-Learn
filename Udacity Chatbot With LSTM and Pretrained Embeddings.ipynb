{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchdata==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import torchtext.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags to avoid repeat work\n",
    "\n",
    "get_embeddings = False\n",
    "get_input_data = False\n",
    "augment_embed_data = False\n",
    "preprocess_input_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "embedding_name = 'glove-twitter-100'\n",
    "validation_frac = 0.2\n",
    "sosToken = 'soseq'\n",
    "eosToken = 'eoseq'\n",
    "loader_qty = 1 # Data loading thread quantity\n",
    "layer_count = 1\n",
    "hidden_unit_dim = 256\n",
    "rep_int = 1000 # Samples per status printout\n",
    "val_int = 5000 # Batches per validation (with printout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embeddings\n",
    "\n",
    "if (get_embeddings) == True:\n",
    "    \n",
    "    base_embeddings = gensim.downloader.load(embedding_name)\n",
    "    base_embeddings.save(embedding_name+'.kv')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    base_embeddings = KeyedVectors.load(embedding_name+'.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_input_data == True:\n",
    "\n",
    "    train_squad, dev_squad = torchtext.datasets.SQuAD1()\n",
    "    base_data = []\n",
    "    for dP in train_squad:\n",
    "        for dpAns in dP[2]:\n",
    "            base_data.append((\" \".join([sosToken,dP[0],dP[1],eosToken]),\" \".join([sosToken,dpAns,eosToken])))\n",
    "    \n",
    "    qa_df = pd.DataFrame(base_data,columns = ['qTxt','aTxt'])\n",
    "    qa_df.to_pickle(\"rawQuestAnsData.pkl\")\n",
    "\n",
    "else:\n",
    "    qa_df = pd.read_pickle(\"rawQuestAnsData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sequence boundary tokens, make all keywords lowercase, rebuild and save keyedVectors\n",
    "\n",
    "if augment_embed_data == True:\n",
    "    \n",
    "    # Prepare numpy array to hold new embedding matrix with sosToken and eosToken added as one-hot\n",
    "    aug_words = []\n",
    "    aug_embed = np.zeros((len(base_embeddings.index_to_key)+2,len(base_embeddings[0])+2))\n",
    "    for i in range(len(base_embeddings.index_to_key)):\n",
    "        aug_words.append(base_embeddings.index_to_key[i].lower())\n",
    "        aug_embed[i,:-2] = base_embeddings[base_embeddings.index_to_key[i]]\n",
    "    \n",
    "    # Add sosToken and eosToken\n",
    "    aug_words.append(sosToken)\n",
    "    aug_embed[-2,-2:] = np.array([1,0])\n",
    "    aug_words.append(eosToken)\n",
    "    aug_embed[-1,-2:] = np.array([0,1])\n",
    "    \n",
    "    # Create new KeyedVectors instance with the extra dimensions for sos, eos\n",
    "    aug_kv = KeyedVectors(aug_embed.shape[1],aug_embed.shape[0])\n",
    "    aug_kv.add_vectors(aug_words,aug_embed)\n",
    "    \n",
    "    # aug_kv.unit_normalize_all()\n",
    "    \n",
    "    # Save\n",
    "    aug_kv.save(embedding_name+'-aug.kv')\n",
    "    \n",
    "else:\n",
    "    aug_kv = KeyedVectors.load(embedding_name+'-aug.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "# Pre-Reqs\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Remove tokens from list not present in embedding\n",
    "def scrubTokens(inTokenList,emb_kv):\n",
    "    outList = [ token for token in inTokenList if (emb_kv.has_index_for(token)) ]\n",
    "    return outList\n",
    "\n",
    "# Tokenization\n",
    "def prepare_text(sentence,emb_kv):\n",
    "    tokens = scrubTokens(word_tokenize(sentence),emb_kv)\n",
    "    return tokens\n",
    "\n",
    "# Prepend to token list\n",
    "def token_prepend(inList,preItem):\n",
    "    return [preItem] + inList\n",
    "\n",
    "# Append to token list\n",
    "def token_append(inList,postItem):\n",
    "    return inList + [postItem]\n",
    "\n",
    "# Transform list of tokens to their indices in embedding\n",
    "def tokensToIndices(tokens,emb_kv):\n",
    "    tokenInds = [emb_kv.get_index(token) for token in tokens]\n",
    "    return tokenInds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_ans(token_list,repeat_times):\n",
    "    out_list = []\n",
    "    out_list.append(token_list[0])\n",
    "    out_list += repeat_times*token_list[1:-1]\n",
    "    out_list.append(token_list[-1])\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize strings\n",
    "# Remove tokens that are not in the embedding\n",
    "# Perform training and validation split\n",
    "# Prepare new KeyedVectors that does not have tokens absent from training data\n",
    "\n",
    "if preprocess_input_data == True:\n",
    "    \n",
    "    proc_qa_df = qa_df.copy()\n",
    "    proc_qa_df['qTxtClean'] = proc_qa_df['qTxt'].str.lower().apply(prepare_text, emb_kv = aug_kv)\n",
    "    proc_qa_df['aTxtClean'] = proc_qa_df['aTxt'].str.lower().apply(prepare_text, emb_kv = aug_kv)\n",
    "    \n",
    "    # Remove rows with no answers in vocab\n",
    "    proc_qa_df['aTxtLen'] = proc_qa_df['aTxtClean'].apply(len)\n",
    "    proc_qa_df = proc_qa_df[proc_qa_df['aTxtLen']>2]    \n",
    "    \n",
    "    print(\"Cleaned Text\")\n",
    "    \n",
    "    pr_kv = KeyedVectors(aug_kv[sosToken].size)\n",
    "    \n",
    "    keyDict = {}\n",
    "    for series in [proc_qa_df['qTxtClean'],proc_qa_df['aTxtClean']]:\n",
    "        for index,tokenList in series.items():\n",
    "            for token in tokenList:\n",
    "                if not token in keyDict.keys():\n",
    "                    keyDict[token] = aug_kv[token]\n",
    "    \n",
    "    keyList = []\n",
    "    valList = []\n",
    "    for token in keyDict.keys():\n",
    "        keyList.append(token)\n",
    "        valList.append(keyDict[token])\n",
    "    \n",
    "    pr_kv.add_vectors(keyList,valList)\n",
    "    \n",
    "    print(\"Pruned vocabulary\")\n",
    "    \n",
    "    # Add dataframe columns with tokens by their numerical indices\n",
    "    proc_qa_df['qIdxs'] = proc_qa_df['qTxtClean'].apply(tokensToIndices, emb_kv = pr_kv)\n",
    "    proc_qa_df['aIdxs'] = proc_qa_df['aTxtClean'].apply(tokensToIndices, emb_kv = pr_kv)\n",
    "    \n",
    "    print(\"Converted to indices\")\n",
    "    \n",
    "    # Save tokenized dataframes with training and validation split\n",
    "    training_size = math.floor((1-validation_frac)*proc_qa_df.shape[0])\n",
    "    train_qa_df = proc_qa_df[:training_size]\n",
    "    train_qa_df.to_pickle('tokenizedTrainingData.pkl')\n",
    "    validation_qa_df = proc_qa_df[training_size:]\n",
    "    validation_qa_df.to_pickle('tokenizedValidationData.pkl')\n",
    "    pr_kv.save(embedding_name+'-prn.kv')\n",
    "    print(\"Saved preprocessed data\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    train_qa_df = pd.read_pickle('tokenizedTrainingData.pkl')\n",
    "    validation_qa_df = pd.read_pickle('tokenizedValidationData.pkl')\n",
    "    pr_kv = KeyedVectors.load(embedding_name+'-prn.kv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object\n",
    "\n",
    "class qaWithContextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, questionAndAnswer_df):\n",
    "        self.qa_df = questionAndAnswer_df\n",
    "        self.length = questionAndAnswer_df['qIdxs'].count()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return tuple of (question,answer)\n",
    "        return (torch.tensor(self.qa_df['qIdxs'].iat[idx]),torch.tensor(self.qa_df['aIdxs'].iat[idx]))\n",
    "    \n",
    "# Batch collation - Pad sequences as tensors, add sequence lengths as lists with nested tuple\n",
    "# ( (questionTensor,questionLengthList), (answerTensor,answerLengthList) )\n",
    "def collate_qa_samples(batch):\n",
    "    \n",
    "    # Sort batch tuples by decreasing question length\n",
    "    batch_sorted = batch.copy()\n",
    "    batch_sorted.sort(reverse = True, key = lambda qa_p: qa_p[0].size()[0])\n",
    "    \n",
    "    questionLengths = [qa_pair[0].size()[0] for qa_pair in batch_sorted]\n",
    "    answerLengths = [qa_pair[1].size()[0] for qa_pair in batch_sorted]\n",
    "    \n",
    "    # Pad sequences with 1's.\n",
    "    # during forward() computations\n",
    "    questions = torch.ones([len(questionLengths),max(questionLengths)], dtype=torch.int64)\n",
    "    answers = torch.ones([len(answerLengths),max(answerLengths)], dtype=torch.int64)\n",
    "    \n",
    "    # Copy sequences into output tensor\n",
    "    for i in range(len(batch_sorted)):\n",
    "        questions[i,0:questionLengths[i]] = batch_sorted[i][0]\n",
    "        answers[i,0:answerLengths[i]] = batch_sorted[i][1]\n",
    "    \n",
    "    return ( (questions,questionLengths) , (answers,answerLengths) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and validation datasets\n",
    "# Truncate to 5000 to try and manage training time\n",
    "train_dataset = qaWithContextDataset(train_qa_df[0:5000])\n",
    "validation_dataset = qaWithContextDataset(validation_qa_df[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader creator so batch_size may be varied\n",
    "def make_dataloader(d_set,batch_qty):\n",
    "    return torch.utils.data.DataLoader( \\\n",
    "                       d_set, \\\n",
    "                       batch_qty, \\\n",
    "                       shuffle = True, \\\n",
    "                       num_workers = loader_qty, \\\n",
    "                       collate_fn = collate_qa_samples, \\\n",
    "                       drop_last = True, \\\n",
    "                       persistent_workers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_embed):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        adjusted_hidden = hidden_size if hidden_size > pretrained_embed.size(dim=1) else pretrained_embed.size(dim=1)\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding( \\\n",
    "                                     num_embeddings = pretrained_embed.size(dim=0), \\\n",
    "                                     embedding_dim = adjusted_hidden, \\\n",
    "                                     )\n",
    "        \n",
    "        # initialize weights for encoder embedding, loading pretrained, expand hidden size if less than pretrained dim\n",
    "        \n",
    "        init_weights = torch.randn(pretrained_embed.size(dim=0),adjusted_hidden)\n",
    "        init_weights[:,:pretrained_embed.size(dim=1)] = pretrained_embed\n",
    "        self.embedding.weight = torch.nn.Parameter(init_weights)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        if (layer_qty > 1):\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True,dropout=0.3)\n",
    "        else:\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src tuple (questions, questionLengths) for batch\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        # Shape of i[0] is [batch_size,sequence_length]\n",
    "        \n",
    "        max_question_length = max(i[1])-1 \n",
    "        \n",
    "        # Get permutation order for sorting questions by decreasing length\n",
    "        sorted_ques_lengths = []\n",
    "        for idx in range(len(i[1])):\n",
    "            sorted_ques_lengths.append((i[1][idx],idx))\n",
    "        sorted_ques_lengths.sort(reverse = True, key = lambda ansLen : ansLen[0])\n",
    "        \n",
    "        # Split up for easier downstream ops\n",
    "        sorted_dec_lengths = [quesLen[0] for quesLen in sorted_ques_lengths]\n",
    "        sorted_idx_list = [quesLen[1] for quesLen in sorted_ques_lengths]\n",
    "        \n",
    "        # Sort the answer padded tensor by decreasing answer length\n",
    "        sorted_idx_tensor = i[0].new_tensor(sorted_idx_list,dtype=torch.int64)\n",
    "        decreasing_length_questions = torch.index_select(i[0],0,sorted_idx_tensor)\n",
    "        \n",
    "        # Shape of embed_rslt is [batch_size,sequence_length,embedding_dim]\n",
    "        embed_rslt = self.embedding(decreasing_length_questions)\n",
    "        \n",
    "        # Encoder does not require online data substitution.  Can use sequence packing functionality.\n",
    "        packed_questions = torch.nn.utils.rnn.pack_padded_sequence(embed_rslt,i[1],batch_first=True,enforce_sorted=True)\n",
    "        \n",
    "        o,(h,c) = self.lstm(packed_questions)\n",
    "        \n",
    "        # Need to undo permutation so questions and answers remain aligned. Output of encoder is unused\n",
    "        inverted_sort_list = [sorted_idx_list.index(old_idx) for old_idx in range(len(sorted_idx_list))]\n",
    "        inverted_sort_tensor = i[0].new_tensor(inverted_sort_list,dtype=torch.int64)\n",
    "        h = torch.index_select(h,1,inverted_sort_tensor)\n",
    "        c = torch.index_select(c,1,inverted_sort_tensor)\n",
    "        \n",
    "        # h.shape == c.shape == [num_layers, batch_size, hidden_size]\n",
    "        return o, (h, c)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_embed):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        adjusted_hidden = hidden_size if hidden_size > pretrained_embed.size(dim=1) else pretrained_embed.size(dim=1)\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding( \\\n",
    "                                     num_embeddings = pretrained_embed.size(dim=0), \\\n",
    "                                     embedding_dim = adjusted_hidden, \\\n",
    "                                     )\n",
    "        \n",
    "        # initialize weights for encoder embedding, loading pretrained, expand hidden size if less than pretrained dim\n",
    "        init_weights = torch.randn(pretrained_embed.size(dim=0),adjusted_hidden)\n",
    "        init_weights[:,:pretrained_embed.size(dim=1)] = pretrained_embed\n",
    "        self.embedding.weight = torch.nn.Parameter(init_weights)\n",
    "        \n",
    "        # Output dimension used to construct outputs\n",
    "        self.out_dim = pretrained_embed.size()[0]\n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        if (layer_qty > 1):\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True,dropout=0.3)\n",
    "        else:\n",
    "            self.lstm = \\\n",
    "                nn.LSTM(adjusted_hidden,adjusted_hidden,num_layers = layer_qty,batch_first=True)\n",
    "        \n",
    "        # self.output, predicts on the LSTM output with linear layer\n",
    "        self.output = nn.Linear(adjusted_hidden,pretrained_embed.size()[0])\n",
    "        self.lsftmx = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, i, enc_state, teach_freq):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target tuple (answers, answerLengths) for batch\n",
    "        Outputs: o, the prediction\n",
    "        '''\n",
    "\n",
    "        are_teaching = True if random.random() < teach_freq else False\n",
    "        max_answer_length = max(i[1])-1 \n",
    "        \n",
    "        # Get permutation order for sorting answers by decreasing length\n",
    "        # Each answer length is reduced by one since the eos token is only used in loss calculation\n",
    "        sorted_ans_lengths = []\n",
    "        for idx in range(len(i[1])):\n",
    "            sorted_ans_lengths.append((i[1][idx]-1,idx))\n",
    "        sorted_ans_lengths.sort(reverse = True, key = lambda ansLen : ansLen[0])\n",
    "        \n",
    "        # Split up for easier downstream ops\n",
    "        sorted_dec_lengths = [ansLen[0] for ansLen in sorted_ans_lengths]\n",
    "        sorted_idx_list = [ansLen[1] for ansLen in sorted_ans_lengths]\n",
    "        \n",
    "        # Sort the answer padded tensor by decreasing answer length\n",
    "        sorted_idx_tensor = i[0].new_tensor(sorted_idx_list,dtype=torch.int64)\n",
    "        decreasing_length_answers = torch.index_select(i[0],0,sorted_idx_tensor)\n",
    "        \n",
    "        # Allocate tensor for predictions\n",
    "        out_preds = i[0].new_ones([len(i[1]),max_answer_length,self.out_dim])\n",
    "        \n",
    "        # Encoder states need to be permuted in same manner as answers (decreasing answer length)\n",
    "        h_enc_decreasing_ans_length = torch.index_select(enc_state[0],1,sorted_idx_tensor)\n",
    "        c_enc_decreasing_ans_length = torch.index_select(enc_state[1],1,sorted_idx_tensor)\n",
    "        \n",
    "        # Teaching: \n",
    "        #  -Can pack answer sequence and run full sequence with single lstm call\n",
    "        # Not Teaching: \n",
    "        #  -Step one token at a time, feeding previous prediction tokens as input.\n",
    "        #  -Don't run sequences through decoder beyond their labeled answer length\n",
    "        if are_teaching:\n",
    "               \n",
    "            embed_rslt = self.embedding(decreasing_length_answers[:,:-1]) # Do not pass eos token\n",
    "            packed_answers = torch.nn.utils.rnn.pack_padded_sequence( \\\n",
    "                embed_rslt,sorted_dec_lengths,batch_first=True,enforce_sorted=True)\n",
    "            teach_pred_out,(h_decode,c_decode) = self.lstm( \\\n",
    "                                                            packed_answers, \\\n",
    "                                                            (h_enc_decreasing_ans_length,c_enc_decreasing_ans_length) \\\n",
    "                                                          )\n",
    "            unpacked_ans, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(teach_pred_out,batch_first=True)\n",
    "            \n",
    "            # Pass through linear layer\n",
    "            out_preds = self.output(unpacked_ans)\n",
    "            out_preds = self.lsftmx(out_preds)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            prev_h = h_enc_decreasing_ans_length\n",
    "            prev_c = c_enc_decreasing_ans_length\n",
    "            \n",
    "            # feed in sos token on first iteration to prime\n",
    "            prev_o = torch.unsqueeze(self.embedding(decreasing_length_answers[:,0]),1)\n",
    "            \n",
    "            # Accumulator list and alias\n",
    "            out_pred_list = []\n",
    "            total_seq = len(sorted_dec_lengths)\n",
    "            \n",
    "            for step in range(max_answer_length):\n",
    "                \n",
    "                # Ensure prepped for gpu run\n",
    "                prev_h = prev_h.contiguous()\n",
    "                prev_c = prev_c.contiguous()\n",
    "                prev_o = prev_o.contiguous()\n",
    "                \n",
    "                # Determine number of sequences that still have remaining predictions to make\n",
    "                seq_left = sum([(1 if step <= ans_len else 0) for ans_len in sorted_dec_lengths])\n",
    "                ltsm_out, (prev_h,prev_c) = \\\n",
    "                    self.lstm( \\\n",
    "                        prev_o[:seq_left,:,:].contiguous(), \\\n",
    "                        (prev_h[:,:seq_left,:].contiguous(),prev_c[:,:seq_left,:].contiguous()))\n",
    "                net_out = self.output(ltsm_out)\n",
    "                net_out = self.lsftmx(net_out)\n",
    "                prev_o = self.embedding(torch.argmax(net_out,dim=2))\n",
    "                \n",
    "                # Add tensor with outputs for seq_left sequences padded \n",
    "                out_pred_list.append( \\\n",
    "                     torch.cat([net_out,i[0].new_ones([total_seq-seq_left,1,self.out_dim])], dim = 0) \\\n",
    "                )\n",
    "        \n",
    "            # Concatenate all the sequence outputs from every sequence step\n",
    "            out_preds = torch.cat(out_pred_list,dim=1)\n",
    "        \n",
    "        # Reorder predictions to original order for loss computation\n",
    "        inverted_sort_list = [sorted_idx_list.index(old_idx) for old_idx in range(len(sorted_idx_list))]\n",
    "        o = torch.index_select(out_preds,0,i[0].new_tensor(inverted_sort_list,dtype=torch.int64))\n",
    "        \n",
    "        return o\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, layer_qty, pretrained_kv):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        # Convert keyedvector's numpy array to tensor\n",
    "        pretrained_embed = torch.tensor(pretrained_kv.vectors,dtype=torch.float32)\n",
    "        \n",
    "        self.seq2seqEncoder = Encoder(hidden_size, layer_qty, pretrained_embed)\n",
    "        self.seq2seqDecoder = Decoder(hidden_size, layer_qty, pretrained_embed)\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "        \n",
    "        o, enc_state = self.seq2seqEncoder.forward(src)\n",
    "        o = self.seq2seqDecoder.forward(trg, enc_state, teacher_forcing_ratio)\n",
    "        \n",
    "        return o\n",
    "    \n",
    "    # Pass tensor to embedding\n",
    "    def embed_tensor(self,inTensor):\n",
    "        return self.seq2seqEncoder.embedding(inTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network object\n",
    "\n",
    "# seqToseqNet = Seq2Seq(hidden_unit_dim,layer_count,pr_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss for variable length sequence data\n",
    "# net_output = (batch size, max sequence length, vocab dimension)\n",
    "# target_seqs = (batch size, max sequence length) indices in vocabulary space\n",
    "# target_lengths = list of lengths for each target in batch\n",
    "# loss_criterion = Loss which can be computed on pair of 1d tensors\n",
    "def computeMaskedLoss(net_output,target_seqs,target_lengths,loss_criterion):\n",
    "    \n",
    "    preds = torch.flatten(net_output,start_dim=0,end_dim=1)\n",
    "    targets = target_seqs[:,1:] # Skip sos token\n",
    "    for i in range(len(target_lengths)):\n",
    "        targets[i,(target_lengths[i]-1):] = (-1)*targets.new_ones((1,targets.size(dim=1)-(target_lengths[i]-1)))\n",
    "    \n",
    "    targets = torch.flatten(targets,start_dim=0,end_dim=1)\n",
    "    loss = loss_criterion(preds,targets) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training routine\n",
    "def train_model(net, train_dset, val_dset,ses_lrn = 0.01,ses_tea = 0.5,ses_epochs = 1,ses_batch_size = 16):\n",
    "\n",
    "    train_loader = make_dataloader(train_dset,ses_batch_size)\n",
    "    val_loader = make_dataloader(val_dset,ses_batch_size)\n",
    "    report_interval = rep_int // ses_batch_size\n",
    "    validation_interval = val_int // ses_batch_size\n",
    "    \n",
    "    least_validation_loss = float(\"inf\")\n",
    "    report_interval_counter = 0\n",
    "    validation_interval_counter = 0\n",
    "    val_iter = iter(val_loader)\n",
    "    s = next(val_iter)\n",
    "    \n",
    "    gpu_avail = torch.cuda.is_available()\n",
    "    \n",
    "    if (gpu_avail):\n",
    "        net.cuda()\n",
    "    \n",
    "    loss_criterion = nn.NLLLoss(ignore_index=-1) # Batches padded with -1's\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=ses_lrn)\n",
    "    \n",
    "    for epoch in range(ses_epochs):\n",
    "        \n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, train_data in enumerate(train_loader):\n",
    "            \n",
    "            train_inputs, train_labels = train_data\n",
    "            \n",
    "            if (gpu_avail):\n",
    "                train_inputs = (train_inputs[0].cuda(), train_inputs[1]) \n",
    "                train_labels = (train_labels[0].cuda(), train_labels[1])\n",
    "            \n",
    "            # Zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model outputs\n",
    "            train_outputs = net(train_inputs,train_labels,teacher_forcing_ratio=ses_tea)\n",
    "            \n",
    "            # Compute loss\n",
    "            train_loss = computeMaskedLoss(train_outputs,train_labels[0],train_labels[1],loss_criterion)\n",
    "            \n",
    "            # Compute the loss gradient using the backward method and have the optimizer take a step\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Report status if is time\n",
    "            report_interval_counter += 1\n",
    "            if report_interval_counter >= report_interval:\n",
    "                report_interval_counter = 0\n",
    "                print(f\"{i+1} batches of epoch {epoch+1} completed.  Last Training Loss: {train_loss: .6f}\")\n",
    "            \n",
    "            # Perform validation run if is time\n",
    "            validation_interval_counter += 1\n",
    "            if validation_interval_counter >= validation_interval:\n",
    "                validation_interval_counter = 0\n",
    "                val_loss = 0.0\n",
    "                net.eval()\n",
    "                \n",
    "                # Get validation batch and evaluate model\n",
    "                # Need try block to handle iterator terminating. See\n",
    "                # https://github.com/pytorch/pytorch/issues/1917#issuecomment-433698337\n",
    "                try:\n",
    "                    val_inputs, val_labels = next(val_iter)\n",
    "                except StopIteration:\n",
    "                    val_iter = iter(val_loader)\n",
    "                    val_inputs, val_labels = next(val_iter)\n",
    "                \n",
    "                if (gpu_avail):\n",
    "                    val_inputs = (val_inputs[0].cuda(), val_inputs[1]) \n",
    "                    val_labels = (val_labels[0].cuda(), val_labels[1])\n",
    "                \n",
    "                # Evaluate validation batch outputs against labels\n",
    "                val_outputs = net(val_inputs, val_labels,teacher_forcing_ratio=0)\n",
    "                \n",
    "                # Compute loss\n",
    "                val_loss = computeMaskedLoss(val_outputs,val_labels[0],val_labels[1],loss_criterion)\n",
    "                \n",
    "                # Update min val loss\n",
    "                if val_loss < least_validation_loss:\n",
    "                    least_validation_loss = val_loss\n",
    "                    print(\"Saving model . . .\")\n",
    "                    torch.save(net,\"Min-Validation-Loss-Model-512.pt\")\n",
    "    \n",
    "                # Report\n",
    "                print(f\"Last Validation Loss: {val_loss: .6f}, Lowest Validation Loss: {least_validation_loss: .6f}\")\n",
    "        \n",
    "                # Cleanup after validation\n",
    "                net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  9.962809\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  7.303122\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  7.047356\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  6.844414\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  6.639485\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.582885, Lowest Validation Loss:  7.582885\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  5.755796\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  5.765522\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  5.990032\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  6.128414\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  6.042053\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  5.783677\n",
      "Last Validation Loss:  7.887328, Lowest Validation Loss:  7.582885\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  5.536327\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  5.603015\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  5.580604\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  5.681340\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  5.649275\n",
      "Last Validation Loss:  7.722011, Lowest Validation Loss:  7.582885\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  5.591450\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  5.660573\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  5.615965\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  5.662284\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  5.483806\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  5.407262\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.503536, Lowest Validation Loss:  7.503536\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  5.362649\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  5.318446\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  5.460412\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  5.428670\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  5.443659\n",
      "Last Validation Loss:  7.549154, Lowest Validation Loss:  7.503536\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  5.367904\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  5.069926\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  5.090133\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  5.298956\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  5.325834\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  5.102356\n",
      "Last Validation Loss:  8.011891, Lowest Validation Loss:  7.503536\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  5.009160\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  4.957948\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  4.990829\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  4.987042\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  4.972895\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  4.946142\n",
      "Last Validation Loss:  7.967293, Lowest Validation Loss:  7.503536\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  4.991176\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  4.760740\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  4.823303\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  4.800942\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  4.880145\n",
      "Last Validation Loss:  7.907057, Lowest Validation Loss:  7.503536\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  4.752233\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  4.657859\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  4.783527\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  4.747986\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  4.891397\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  4.695115\n",
      "Last Validation Loss:  8.336551, Lowest Validation Loss:  7.503536\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  4.521399\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  4.609879\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  4.539632\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  4.695903\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  4.646302\n",
      "Last Validation Loss:  8.061272, Lowest Validation Loss:  7.503536\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  4.337636\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  4.328249\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  4.317931\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  4.487329\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  4.503613\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  4.356325\n",
      "Last Validation Loss:  8.481414, Lowest Validation Loss:  7.503536\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  4.412927\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  4.300396\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  4.516693\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  4.238823\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  4.300164\n",
      "Last Validation Loss:  8.487987, Lowest Validation Loss:  7.503536\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  4.232100\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  4.048850\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  4.200999\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  4.284172\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  4.348583\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  4.271849\n",
      "Last Validation Loss:  8.547383, Lowest Validation Loss:  7.503536\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  4.004086\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  3.987189\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  4.115223\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  4.111816\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  4.216947\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  4.186708\n",
      "Last Validation Loss:  8.404757, Lowest Validation Loss:  7.503536\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  3.963478\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  3.852918\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  3.798888\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  3.936918\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  4.063375\n",
      "Last Validation Loss:  8.590150, Lowest Validation Loss:  7.503536\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  3.845326\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  3.812591\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  3.808901\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  3.791724\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  3.893442\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  3.974404\n",
      "Last Validation Loss:  9.163886, Lowest Validation Loss:  7.503536\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  3.806373\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  3.694645\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  3.691132\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  3.724043\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  3.858039\n",
      "Last Validation Loss:  9.220275, Lowest Validation Loss:  7.503536\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  3.564669\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  3.547710\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  3.640075\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  3.554424\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  3.511600\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  3.575341\n",
      "Last Validation Loss:  9.436028, Lowest Validation Loss:  7.503536\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  3.451309\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  3.421903\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  3.344814\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  3.412830\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  3.468960\n",
      "Last Validation Loss:  9.259890, Lowest Validation Loss:  7.503536\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  3.348706\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  3.262733\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  3.314983\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  3.314932\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  3.434432\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  3.406321\n",
      "Last Validation Loss:  9.556064, Lowest Validation Loss:  7.503536\n"
     ]
    }
   ],
   "source": [
    "# Perform first training run\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.001, ses_tea = 1.0, ses_epochs = 20, ses_batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  3.234920\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  3.202101\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  3.233509\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  3.348517\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  3.252787\n",
      "Saving model . . .\n",
      "Last Validation Loss:  9.689758, Lowest Validation Loss:  9.689758\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  3.150636\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  3.329682\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  3.153099\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  3.233053\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  3.143806\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  3.119638\n",
      "Last Validation Loss:  9.699584, Lowest Validation Loss:  9.689758\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  3.147513\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  3.060886\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  3.268667\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  3.061564\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  3.280957\n",
      "Saving model . . .\n",
      "Last Validation Loss:  9.592052, Lowest Validation Loss:  9.592052\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  3.042347\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  3.130527\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  3.205510\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  3.150291\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  3.117479\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  3.121511\n",
      "Last Validation Loss:  9.791123, Lowest Validation Loss:  9.592052\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  3.197710\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  3.128441\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  3.122897\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  3.098657\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  3.111409\n",
      "Last Validation Loss:  9.793965, Lowest Validation Loss:  9.592052\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  3.159772\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  3.198437\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  3.137945\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  2.984588\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  3.095703\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  2.972283\n",
      "Last Validation Loss:  9.880480, Lowest Validation Loss:  9.592052\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  3.114769\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  3.081247\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  3.030131\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  3.103605\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  3.066530\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  3.125362\n",
      "Last Validation Loss:  9.758440, Lowest Validation Loss:  9.592052\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  3.053020\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  3.036748\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  3.006976\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  3.038630\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  3.116318\n",
      "Last Validation Loss:  10.515599, Lowest Validation Loss:  9.592052\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  2.830368\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  3.062065\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  2.985730\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  3.060739\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  3.023126\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  3.077618\n",
      "Last Validation Loss:  10.082081, Lowest Validation Loss:  9.592052\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  3.055601\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  2.915501\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  3.049486\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  3.029283\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  3.069727\n",
      "Last Validation Loss:  10.541773, Lowest Validation Loss:  9.592052\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  2.942731\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  3.012825\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  2.943928\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  3.058855\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  3.095480\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  3.031585\n",
      "Last Validation Loss:  10.322147, Lowest Validation Loss:  9.592052\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  3.075887\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  3.052365\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  3.017016\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  3.061608\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  2.976027\n",
      "Last Validation Loss:  10.498096, Lowest Validation Loss:  9.592052\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  3.019718\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  2.960993\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  2.888695\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  2.842531\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  3.029002\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  3.024432\n",
      "Last Validation Loss:  10.858777, Lowest Validation Loss:  9.592052\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  2.977159\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  2.871785\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  2.993412\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  2.902539\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  2.948597\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  2.972849\n",
      "Last Validation Loss:  10.448407, Lowest Validation Loss:  9.592052\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  2.970209\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  2.854769\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  2.893724\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  2.970165\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  2.938088\n",
      "Last Validation Loss:  10.859182, Lowest Validation Loss:  9.592052\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  2.862046\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  2.905044\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  2.918418\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  2.990600\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  2.872668\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  2.850282\n",
      "Last Validation Loss:  10.451365, Lowest Validation Loss:  9.592052\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  2.913172\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  3.037408\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  2.866066\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  2.873518\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  2.908609\n",
      "Last Validation Loss:  10.929676, Lowest Validation Loss:  9.592052\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  2.850587\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  2.850898\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  2.876665\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  3.136327\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  2.870085\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  2.904328\n",
      "Last Validation Loss:  10.648694, Lowest Validation Loss:  9.592052\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  3.010489\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  2.914284\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  2.813752\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  2.808173\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  2.883361\n",
      "Last Validation Loss:  11.086279, Lowest Validation Loss:  9.592052\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  2.888091\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  2.898839\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  2.854193\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  2.729934\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  2.922570\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  2.875932\n",
      "Last Validation Loss:  10.391525, Lowest Validation Loss:  9.592052\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunOne.pt')\n",
    "# Second training run.  Lower learning rate to 0.0001, keep teach at 1.0.\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.0001, ses_tea = 1.0, ses_epochs = 20, ses_batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 batches of epoch 1 completed.  Last Training Loss:  2.844584\n",
      "14 batches of epoch 1 completed.  Last Training Loss:  2.900691\n",
      "21 batches of epoch 1 completed.  Last Training Loss:  3.041736\n",
      "28 batches of epoch 1 completed.  Last Training Loss:  2.972529\n",
      "35 batches of epoch 1 completed.  Last Training Loss:  3.015214\n",
      "Saving model . . .\n",
      "Last Validation Loss:  10.372571, Lowest Validation Loss:  10.372571\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  2.777930\n",
      "10 batches of epoch 2 completed.  Last Training Loss:  2.867497\n",
      "17 batches of epoch 2 completed.  Last Training Loss:  2.907537\n",
      "24 batches of epoch 2 completed.  Last Training Loss:  2.807493\n",
      "31 batches of epoch 2 completed.  Last Training Loss:  2.879120\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  2.900671\n",
      "Last Validation Loss:  10.401089, Lowest Validation Loss:  10.372571\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  2.755357\n",
      "13 batches of epoch 3 completed.  Last Training Loss:  2.728160\n",
      "20 batches of epoch 3 completed.  Last Training Loss:  2.756908\n",
      "27 batches of epoch 3 completed.  Last Training Loss:  2.832523\n",
      "34 batches of epoch 3 completed.  Last Training Loss:  2.687571\n",
      "Last Validation Loss:  10.657236, Lowest Validation Loss:  10.372571\n",
      "2 batches of epoch 4 completed.  Last Training Loss:  2.697782\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  2.578264\n",
      "16 batches of epoch 4 completed.  Last Training Loss:  2.866564\n",
      "23 batches of epoch 4 completed.  Last Training Loss:  2.657322\n",
      "30 batches of epoch 4 completed.  Last Training Loss:  2.677502\n",
      "37 batches of epoch 4 completed.  Last Training Loss:  2.783835\n",
      "Last Validation Loss:  11.350740, Lowest Validation Loss:  10.372571\n",
      "5 batches of epoch 5 completed.  Last Training Loss:  2.575228\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  2.598364\n",
      "19 batches of epoch 5 completed.  Last Training Loss:  2.450586\n",
      "26 batches of epoch 5 completed.  Last Training Loss:  2.612386\n",
      "33 batches of epoch 5 completed.  Last Training Loss:  2.608242\n",
      "Last Validation Loss:  11.437814, Lowest Validation Loss:  10.372571\n",
      "1 batches of epoch 6 completed.  Last Training Loss:  2.459729\n",
      "8 batches of epoch 6 completed.  Last Training Loss:  2.430357\n",
      "15 batches of epoch 6 completed.  Last Training Loss:  2.569813\n",
      "22 batches of epoch 6 completed.  Last Training Loss:  2.531808\n",
      "29 batches of epoch 6 completed.  Last Training Loss:  2.606745\n",
      "36 batches of epoch 6 completed.  Last Training Loss:  2.665014\n",
      "Last Validation Loss:  11.856577, Lowest Validation Loss:  10.372571\n",
      "4 batches of epoch 7 completed.  Last Training Loss:  2.586174\n",
      "11 batches of epoch 7 completed.  Last Training Loss:  2.407112\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  2.457397\n",
      "25 batches of epoch 7 completed.  Last Training Loss:  2.594963\n",
      "32 batches of epoch 7 completed.  Last Training Loss:  2.611619\n",
      "39 batches of epoch 7 completed.  Last Training Loss:  2.453870\n",
      "Last Validation Loss:  11.714125, Lowest Validation Loss:  10.372571\n",
      "7 batches of epoch 8 completed.  Last Training Loss:  2.406635\n",
      "14 batches of epoch 8 completed.  Last Training Loss:  2.414643\n",
      "21 batches of epoch 8 completed.  Last Training Loss:  2.468953\n",
      "28 batches of epoch 8 completed.  Last Training Loss:  2.500128\n",
      "35 batches of epoch 8 completed.  Last Training Loss:  2.555052\n",
      "Last Validation Loss:  12.961480, Lowest Validation Loss:  10.372571\n",
      "3 batches of epoch 9 completed.  Last Training Loss:  2.395342\n",
      "10 batches of epoch 9 completed.  Last Training Loss:  2.428732\n",
      "17 batches of epoch 9 completed.  Last Training Loss:  2.421233\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  2.471477\n",
      "31 batches of epoch 9 completed.  Last Training Loss:  2.357694\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  2.369823\n",
      "Last Validation Loss:  11.995814, Lowest Validation Loss:  10.372571\n",
      "6 batches of epoch 10 completed.  Last Training Loss:  2.351227\n",
      "13 batches of epoch 10 completed.  Last Training Loss:  2.372884\n",
      "20 batches of epoch 10 completed.  Last Training Loss:  2.366398\n",
      "27 batches of epoch 10 completed.  Last Training Loss:  2.281016\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  2.457664\n",
      "Last Validation Loss:  12.522483, Lowest Validation Loss:  10.372571\n",
      "2 batches of epoch 11 completed.  Last Training Loss:  2.226580\n",
      "9 batches of epoch 11 completed.  Last Training Loss:  2.246932\n",
      "16 batches of epoch 11 completed.  Last Training Loss:  2.284094\n",
      "23 batches of epoch 11 completed.  Last Training Loss:  2.333406\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  2.288316\n",
      "37 batches of epoch 11 completed.  Last Training Loss:  2.308821\n",
      "Last Validation Loss:  12.341218, Lowest Validation Loss:  10.372571\n",
      "5 batches of epoch 12 completed.  Last Training Loss:  2.265023\n",
      "12 batches of epoch 12 completed.  Last Training Loss:  2.206277\n",
      "19 batches of epoch 12 completed.  Last Training Loss:  2.177628\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  2.254256\n",
      "33 batches of epoch 12 completed.  Last Training Loss:  2.253572\n",
      "Last Validation Loss:  12.823518, Lowest Validation Loss:  10.372571\n",
      "1 batches of epoch 13 completed.  Last Training Loss:  2.220867\n",
      "8 batches of epoch 13 completed.  Last Training Loss:  2.140703\n",
      "15 batches of epoch 13 completed.  Last Training Loss:  2.128756\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  2.260930\n",
      "29 batches of epoch 13 completed.  Last Training Loss:  2.242954\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  2.294530\n",
      "Last Validation Loss:  12.737585, Lowest Validation Loss:  10.372571\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  2.218941\n",
      "11 batches of epoch 14 completed.  Last Training Loss:  2.113653\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  2.245923\n",
      "25 batches of epoch 14 completed.  Last Training Loss:  2.231203\n",
      "32 batches of epoch 14 completed.  Last Training Loss:  2.085305\n",
      "39 batches of epoch 14 completed.  Last Training Loss:  2.199822\n",
      "Last Validation Loss:  12.843524, Lowest Validation Loss:  10.372571\n",
      "7 batches of epoch 15 completed.  Last Training Loss:  2.059244\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  1.980080\n",
      "21 batches of epoch 15 completed.  Last Training Loss:  2.082928\n",
      "28 batches of epoch 15 completed.  Last Training Loss:  2.181412\n",
      "35 batches of epoch 15 completed.  Last Training Loss:  2.078587\n",
      "Last Validation Loss:  12.907781, Lowest Validation Loss:  10.372571\n",
      "3 batches of epoch 16 completed.  Last Training Loss:  2.217992\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  1.999721\n",
      "17 batches of epoch 16 completed.  Last Training Loss:  1.982387\n",
      "24 batches of epoch 16 completed.  Last Training Loss:  2.080773\n",
      "31 batches of epoch 16 completed.  Last Training Loss:  2.129610\n",
      "38 batches of epoch 16 completed.  Last Training Loss:  1.986547\n",
      "Last Validation Loss:  13.208985, Lowest Validation Loss:  10.372571\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  1.988808\n",
      "13 batches of epoch 17 completed.  Last Training Loss:  1.916615\n",
      "20 batches of epoch 17 completed.  Last Training Loss:  2.035812\n",
      "27 batches of epoch 17 completed.  Last Training Loss:  2.100245\n",
      "34 batches of epoch 17 completed.  Last Training Loss:  1.954622\n",
      "Last Validation Loss:  13.091337, Lowest Validation Loss:  10.372571\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  1.923213\n",
      "9 batches of epoch 18 completed.  Last Training Loss:  1.907067\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  1.859777\n",
      "23 batches of epoch 18 completed.  Last Training Loss:  1.989355\n",
      "30 batches of epoch 18 completed.  Last Training Loss:  2.014968\n",
      "37 batches of epoch 18 completed.  Last Training Loss:  2.046722\n",
      "Last Validation Loss:  13.088575, Lowest Validation Loss:  10.372571\n",
      "5 batches of epoch 19 completed.  Last Training Loss:  1.748379\n",
      "12 batches of epoch 19 completed.  Last Training Loss:  1.762795\n",
      "19 batches of epoch 19 completed.  Last Training Loss:  1.892708\n",
      "26 batches of epoch 19 completed.  Last Training Loss:  1.932428\n",
      "33 batches of epoch 19 completed.  Last Training Loss:  1.860201\n",
      "Last Validation Loss:  13.139251, Lowest Validation Loss:  10.372571\n",
      "1 batches of epoch 20 completed.  Last Training Loss:  1.909822\n",
      "8 batches of epoch 20 completed.  Last Training Loss:  1.768922\n",
      "15 batches of epoch 20 completed.  Last Training Loss:  1.872094\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  1.849902\n",
      "29 batches of epoch 20 completed.  Last Training Loss:  1.910452\n",
      "36 batches of epoch 20 completed.  Last Training Loss:  1.882176\n",
      "Last Validation Loss:  13.458951, Lowest Validation Loss:  10.372571\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunTwo.pt')\n",
    "# Third training run.  Raise learning rate to 0.0005, keep teach at 1.0.\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.0005, ses_tea = 1.0, ses_epochs = 20, ses_batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 batches of epoch 1 completed.  Last Training Loss:  1.754943\n",
      "20 batches of epoch 1 completed.  Last Training Loss:  1.693097\n",
      "30 batches of epoch 1 completed.  Last Training Loss:  1.807624\n",
      "40 batches of epoch 1 completed.  Last Training Loss:  1.780202\n",
      "50 batches of epoch 1 completed.  Last Training Loss:  1.713104\n",
      "Saving model . . .\n",
      "Last Validation Loss:  13.993649, Lowest Validation Loss:  13.993649\n",
      "8 batches of epoch 2 completed.  Last Training Loss:  1.529749\n",
      "18 batches of epoch 2 completed.  Last Training Loss:  1.568932\n",
      "28 batches of epoch 2 completed.  Last Training Loss:  1.645175\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  1.647937\n",
      "48 batches of epoch 2 completed.  Last Training Loss:  1.673804\n",
      "Saving model . . .\n",
      "Last Validation Loss:  12.735620, Lowest Validation Loss:  12.735620\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  1.575722\n",
      "16 batches of epoch 3 completed.  Last Training Loss:  1.355341\n",
      "26 batches of epoch 3 completed.  Last Training Loss:  1.564966\n",
      "36 batches of epoch 3 completed.  Last Training Loss:  1.542814\n",
      "46 batches of epoch 3 completed.  Last Training Loss:  1.563025\n",
      "Last Validation Loss:  13.264651, Lowest Validation Loss:  12.735620\n",
      "4 batches of epoch 4 completed.  Last Training Loss:  1.489125\n",
      "14 batches of epoch 4 completed.  Last Training Loss:  1.594220\n",
      "24 batches of epoch 4 completed.  Last Training Loss:  1.568034\n",
      "34 batches of epoch 4 completed.  Last Training Loss:  1.663719\n",
      "44 batches of epoch 4 completed.  Last Training Loss:  1.525237\n",
      "Last Validation Loss:  13.938406, Lowest Validation Loss:  12.735620\n",
      "2 batches of epoch 5 completed.  Last Training Loss:  1.298916\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  1.490334\n",
      "22 batches of epoch 5 completed.  Last Training Loss:  1.510140\n",
      "32 batches of epoch 5 completed.  Last Training Loss:  1.388300\n",
      "42 batches of epoch 5 completed.  Last Training Loss:  1.398199\n",
      "52 batches of epoch 5 completed.  Last Training Loss:  1.388875\n",
      "Last Validation Loss:  13.743125, Lowest Validation Loss:  12.735620\n",
      "10 batches of epoch 6 completed.  Last Training Loss:  1.446167\n",
      "20 batches of epoch 6 completed.  Last Training Loss:  1.482007\n",
      "30 batches of epoch 6 completed.  Last Training Loss:  1.461854\n",
      "40 batches of epoch 6 completed.  Last Training Loss:  1.458925\n",
      "50 batches of epoch 6 completed.  Last Training Loss:  1.469445\n",
      "Last Validation Loss:  13.680111, Lowest Validation Loss:  12.735620\n",
      "8 batches of epoch 7 completed.  Last Training Loss:  1.423467\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  1.391789\n",
      "28 batches of epoch 7 completed.  Last Training Loss:  1.402236\n",
      "38 batches of epoch 7 completed.  Last Training Loss:  1.350889\n",
      "48 batches of epoch 7 completed.  Last Training Loss:  1.381713\n",
      "Last Validation Loss:  14.850797, Lowest Validation Loss:  12.735620\n",
      "6 batches of epoch 8 completed.  Last Training Loss:  1.366811\n",
      "16 batches of epoch 8 completed.  Last Training Loss:  1.279180\n",
      "26 batches of epoch 8 completed.  Last Training Loss:  1.330229\n",
      "36 batches of epoch 8 completed.  Last Training Loss:  1.378381\n",
      "46 batches of epoch 8 completed.  Last Training Loss:  1.388866\n",
      "Last Validation Loss:  13.797403, Lowest Validation Loss:  12.735620\n",
      "4 batches of epoch 9 completed.  Last Training Loss:  1.253591\n",
      "14 batches of epoch 9 completed.  Last Training Loss:  1.299892\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  1.358255\n",
      "34 batches of epoch 9 completed.  Last Training Loss:  1.290818\n",
      "44 batches of epoch 9 completed.  Last Training Loss:  1.207901\n",
      "Last Validation Loss:  14.891796, Lowest Validation Loss:  12.735620\n",
      "2 batches of epoch 10 completed.  Last Training Loss:  1.146357\n",
      "12 batches of epoch 10 completed.  Last Training Loss:  1.251157\n",
      "22 batches of epoch 10 completed.  Last Training Loss:  1.225933\n",
      "32 batches of epoch 10 completed.  Last Training Loss:  1.298153\n",
      "42 batches of epoch 10 completed.  Last Training Loss:  1.309281\n",
      "52 batches of epoch 10 completed.  Last Training Loss:  1.240827\n",
      "Last Validation Loss:  16.126982, Lowest Validation Loss:  12.735620\n",
      "10 batches of epoch 11 completed.  Last Training Loss:  1.242767\n",
      "20 batches of epoch 11 completed.  Last Training Loss:  1.296624\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  1.195734\n",
      "40 batches of epoch 11 completed.  Last Training Loss:  1.102228\n",
      "50 batches of epoch 11 completed.  Last Training Loss:  1.302318\n",
      "Last Validation Loss:  14.177702, Lowest Validation Loss:  12.735620\n",
      "8 batches of epoch 12 completed.  Last Training Loss:  1.047446\n",
      "18 batches of epoch 12 completed.  Last Training Loss:  1.250152\n",
      "28 batches of epoch 12 completed.  Last Training Loss:  1.249250\n",
      "38 batches of epoch 12 completed.  Last Training Loss:  1.208382\n",
      "48 batches of epoch 12 completed.  Last Training Loss:  1.172120\n",
      "Last Validation Loss:  14.236147, Lowest Validation Loss:  12.735620\n",
      "6 batches of epoch 13 completed.  Last Training Loss:  1.102731\n",
      "16 batches of epoch 13 completed.  Last Training Loss:  1.039804\n",
      "26 batches of epoch 13 completed.  Last Training Loss:  1.179528\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  1.094781\n",
      "46 batches of epoch 13 completed.  Last Training Loss:  1.198669\n",
      "Last Validation Loss:  15.883671, Lowest Validation Loss:  12.735620\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  1.067327\n",
      "14 batches of epoch 14 completed.  Last Training Loss:  1.057899\n",
      "24 batches of epoch 14 completed.  Last Training Loss:  1.093781\n",
      "34 batches of epoch 14 completed.  Last Training Loss:  1.205620\n",
      "44 batches of epoch 14 completed.  Last Training Loss:  1.206139\n",
      "Last Validation Loss:  14.648339, Lowest Validation Loss:  12.735620\n",
      "2 batches of epoch 15 completed.  Last Training Loss:  1.071683\n",
      "12 batches of epoch 15 completed.  Last Training Loss:  1.090229\n",
      "22 batches of epoch 15 completed.  Last Training Loss:  1.026141\n",
      "32 batches of epoch 15 completed.  Last Training Loss:  1.203249\n",
      "42 batches of epoch 15 completed.  Last Training Loss:  1.094200\n",
      "52 batches of epoch 15 completed.  Last Training Loss:  1.072593\n",
      "Last Validation Loss:  14.276408, Lowest Validation Loss:  12.735620\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  1.024285\n",
      "20 batches of epoch 16 completed.  Last Training Loss:  1.123336\n",
      "30 batches of epoch 16 completed.  Last Training Loss:  1.098345\n",
      "40 batches of epoch 16 completed.  Last Training Loss:  1.197248\n",
      "50 batches of epoch 16 completed.  Last Training Loss:  1.086352\n",
      "Last Validation Loss:  14.196648, Lowest Validation Loss:  12.735620\n",
      "8 batches of epoch 17 completed.  Last Training Loss:  0.931676\n",
      "18 batches of epoch 17 completed.  Last Training Loss:  0.995150\n",
      "28 batches of epoch 17 completed.  Last Training Loss:  0.949967\n",
      "38 batches of epoch 17 completed.  Last Training Loss:  1.132849\n",
      "48 batches of epoch 17 completed.  Last Training Loss:  0.990546\n",
      "Last Validation Loss:  13.970922, Lowest Validation Loss:  12.735620\n",
      "6 batches of epoch 18 completed.  Last Training Loss:  0.917870\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  0.973116\n",
      "26 batches of epoch 18 completed.  Last Training Loss:  0.968414\n",
      "36 batches of epoch 18 completed.  Last Training Loss:  0.985953\n",
      "46 batches of epoch 18 completed.  Last Training Loss:  0.908646\n",
      "Last Validation Loss:  14.984106, Lowest Validation Loss:  12.735620\n",
      "4 batches of epoch 19 completed.  Last Training Loss:  0.950176\n",
      "14 batches of epoch 19 completed.  Last Training Loss:  0.872829\n",
      "24 batches of epoch 19 completed.  Last Training Loss:  0.912912\n",
      "34 batches of epoch 19 completed.  Last Training Loss:  0.868584\n",
      "44 batches of epoch 19 completed.  Last Training Loss:  0.908113\n",
      "Last Validation Loss:  15.008348, Lowest Validation Loss:  12.735620\n",
      "2 batches of epoch 20 completed.  Last Training Loss:  0.880114\n",
      "12 batches of epoch 20 completed.  Last Training Loss:  0.820404\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  0.782666\n",
      "32 batches of epoch 20 completed.  Last Training Loss:  0.896882\n",
      "42 batches of epoch 20 completed.  Last Training Loss:  0.836294\n",
      "52 batches of epoch 20 completed.  Last Training Loss:  0.949523\n",
      "Last Validation Loss:  14.786483, Lowest Validation Loss:  12.735620\n"
     ]
    }
   ],
   "source": [
    "# torch.save(seqToseqNet,'afterRunThree.pt')\n",
    "# Fourth training run.  Keep learning rate at 0.0005, keep teach at 1.0.\n",
    "# Had a cuda memory error so reloaded, decreased batch size to 96\n",
    "seqToseqNet = torch.load('afterRunThree.pt')\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.0005, ses_tea = 1.0, ses_epochs = 20, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 batches of epoch 1 completed.  Last Training Loss:  5.544827\n",
      "20 batches of epoch 1 completed.  Last Training Loss:  5.635997\n",
      "30 batches of epoch 1 completed.  Last Training Loss:  4.969920\n",
      "40 batches of epoch 1 completed.  Last Training Loss:  5.720712\n",
      "50 batches of epoch 1 completed.  Last Training Loss:  5.105931\n",
      "Saving model . . .\n",
      "Last Validation Loss:  9.903163, Lowest Validation Loss:  9.903163\n",
      "8 batches of epoch 2 completed.  Last Training Loss:  4.501165\n",
      "18 batches of epoch 2 completed.  Last Training Loss:  3.681868\n",
      "28 batches of epoch 2 completed.  Last Training Loss:  4.268641\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  4.115623\n",
      "48 batches of epoch 2 completed.  Last Training Loss:  3.652003\n",
      "Last Validation Loss:  10.084297, Lowest Validation Loss:  9.903163\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  3.594330\n",
      "16 batches of epoch 3 completed.  Last Training Loss:  3.061910\n",
      "26 batches of epoch 3 completed.  Last Training Loss:  3.777947\n",
      "36 batches of epoch 3 completed.  Last Training Loss:  3.549745\n",
      "46 batches of epoch 3 completed.  Last Training Loss:  3.556582\n",
      "Last Validation Loss:  9.964820, Lowest Validation Loss:  9.903163\n",
      "4 batches of epoch 4 completed.  Last Training Loss:  3.470274\n",
      "14 batches of epoch 4 completed.  Last Training Loss:  2.995774\n",
      "24 batches of epoch 4 completed.  Last Training Loss:  3.143301\n",
      "34 batches of epoch 4 completed.  Last Training Loss:  3.621516\n",
      "44 batches of epoch 4 completed.  Last Training Loss:  3.328926\n",
      "Last Validation Loss:  10.425511, Lowest Validation Loss:  9.903163\n",
      "2 batches of epoch 5 completed.  Last Training Loss:  3.219502\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  3.039019\n",
      "22 batches of epoch 5 completed.  Last Training Loss:  3.390557\n",
      "32 batches of epoch 5 completed.  Last Training Loss:  2.655245\n",
      "42 batches of epoch 5 completed.  Last Training Loss:  3.295840\n",
      "52 batches of epoch 5 completed.  Last Training Loss:  2.950360\n",
      "Last Validation Loss:  10.512952, Lowest Validation Loss:  9.903163\n",
      "10 batches of epoch 6 completed.  Last Training Loss:  2.942612\n",
      "20 batches of epoch 6 completed.  Last Training Loss:  3.011493\n",
      "30 batches of epoch 6 completed.  Last Training Loss:  2.685456\n",
      "40 batches of epoch 6 completed.  Last Training Loss:  3.212093\n",
      "50 batches of epoch 6 completed.  Last Training Loss:  2.676705\n",
      "Last Validation Loss:  10.826328, Lowest Validation Loss:  9.903163\n",
      "8 batches of epoch 7 completed.  Last Training Loss:  2.553652\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  2.605070\n",
      "28 batches of epoch 7 completed.  Last Training Loss:  2.875996\n",
      "38 batches of epoch 7 completed.  Last Training Loss:  2.760839\n",
      "48 batches of epoch 7 completed.  Last Training Loss:  2.240016\n",
      "Last Validation Loss:  10.378551, Lowest Validation Loss:  9.903163\n",
      "6 batches of epoch 8 completed.  Last Training Loss:  2.304018\n",
      "16 batches of epoch 8 completed.  Last Training Loss:  2.277235\n",
      "26 batches of epoch 8 completed.  Last Training Loss:  2.194106\n",
      "36 batches of epoch 8 completed.  Last Training Loss:  2.739330\n",
      "46 batches of epoch 8 completed.  Last Training Loss:  2.446459\n",
      "Last Validation Loss:  10.471603, Lowest Validation Loss:  9.903163\n",
      "4 batches of epoch 9 completed.  Last Training Loss:  2.192504\n",
      "14 batches of epoch 9 completed.  Last Training Loss:  1.931122\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  2.618739\n",
      "34 batches of epoch 9 completed.  Last Training Loss:  2.673170\n",
      "44 batches of epoch 9 completed.  Last Training Loss:  2.397262\n",
      "Last Validation Loss:  11.162138, Lowest Validation Loss:  9.903163\n",
      "2 batches of epoch 10 completed.  Last Training Loss:  1.995845\n",
      "12 batches of epoch 10 completed.  Last Training Loss:  1.879856\n",
      "22 batches of epoch 10 completed.  Last Training Loss:  2.240734\n",
      "32 batches of epoch 10 completed.  Last Training Loss:  2.246680\n",
      "42 batches of epoch 10 completed.  Last Training Loss:  2.207865\n",
      "52 batches of epoch 10 completed.  Last Training Loss:  2.311747\n",
      "Last Validation Loss:  10.878023, Lowest Validation Loss:  9.903163\n",
      "10 batches of epoch 11 completed.  Last Training Loss:  2.001221\n",
      "20 batches of epoch 11 completed.  Last Training Loss:  1.517951\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  1.714464\n",
      "40 batches of epoch 11 completed.  Last Training Loss:  1.849384\n",
      "50 batches of epoch 11 completed.  Last Training Loss:  2.142203\n",
      "Last Validation Loss:  10.798183, Lowest Validation Loss:  9.903163\n",
      "8 batches of epoch 12 completed.  Last Training Loss:  1.849265\n",
      "18 batches of epoch 12 completed.  Last Training Loss:  1.762940\n",
      "28 batches of epoch 12 completed.  Last Training Loss:  1.756857\n",
      "38 batches of epoch 12 completed.  Last Training Loss:  1.956383\n",
      "48 batches of epoch 12 completed.  Last Training Loss:  2.057757\n",
      "Last Validation Loss:  10.970524, Lowest Validation Loss:  9.903163\n",
      "6 batches of epoch 13 completed.  Last Training Loss:  1.640370\n",
      "16 batches of epoch 13 completed.  Last Training Loss:  1.515611\n",
      "26 batches of epoch 13 completed.  Last Training Loss:  1.620534\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  1.717657\n",
      "46 batches of epoch 13 completed.  Last Training Loss:  1.724232\n",
      "Last Validation Loss:  10.695675, Lowest Validation Loss:  9.903163\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  1.739730\n",
      "14 batches of epoch 14 completed.  Last Training Loss:  1.185222\n",
      "24 batches of epoch 14 completed.  Last Training Loss:  1.688412\n",
      "34 batches of epoch 14 completed.  Last Training Loss:  1.572951\n",
      "44 batches of epoch 14 completed.  Last Training Loss:  1.390630\n",
      "Last Validation Loss:  10.991182, Lowest Validation Loss:  9.903163\n",
      "2 batches of epoch 15 completed.  Last Training Loss:  1.386627\n",
      "12 batches of epoch 15 completed.  Last Training Loss:  1.365343\n",
      "22 batches of epoch 15 completed.  Last Training Loss:  1.191462\n",
      "32 batches of epoch 15 completed.  Last Training Loss:  1.246494\n",
      "42 batches of epoch 15 completed.  Last Training Loss:  1.408500\n",
      "52 batches of epoch 15 completed.  Last Training Loss:  1.175224\n",
      "Last Validation Loss:  11.696254, Lowest Validation Loss:  9.903163\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  1.195806\n",
      "20 batches of epoch 16 completed.  Last Training Loss:  1.164541\n",
      "30 batches of epoch 16 completed.  Last Training Loss:  1.475262\n",
      "40 batches of epoch 16 completed.  Last Training Loss:  1.398778\n",
      "50 batches of epoch 16 completed.  Last Training Loss:  1.455062\n",
      "Last Validation Loss:  11.705352, Lowest Validation Loss:  9.903163\n",
      "8 batches of epoch 17 completed.  Last Training Loss:  0.836961\n",
      "18 batches of epoch 17 completed.  Last Training Loss:  1.044257\n",
      "28 batches of epoch 17 completed.  Last Training Loss:  1.353304\n",
      "38 batches of epoch 17 completed.  Last Training Loss:  1.082417\n",
      "48 batches of epoch 17 completed.  Last Training Loss:  1.025605\n",
      "Last Validation Loss:  11.701120, Lowest Validation Loss:  9.903163\n",
      "6 batches of epoch 18 completed.  Last Training Loss:  0.710512\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  0.914716\n",
      "26 batches of epoch 18 completed.  Last Training Loss:  1.065939\n",
      "36 batches of epoch 18 completed.  Last Training Loss:  1.560745\n",
      "46 batches of epoch 18 completed.  Last Training Loss:  0.960498\n",
      "Last Validation Loss:  11.510111, Lowest Validation Loss:  9.903163\n",
      "4 batches of epoch 19 completed.  Last Training Loss:  1.145584\n",
      "14 batches of epoch 19 completed.  Last Training Loss:  1.057487\n",
      "24 batches of epoch 19 completed.  Last Training Loss:  0.974483\n",
      "34 batches of epoch 19 completed.  Last Training Loss:  0.894061\n",
      "44 batches of epoch 19 completed.  Last Training Loss:  1.002352\n",
      "Last Validation Loss:  12.578394, Lowest Validation Loss:  9.903163\n",
      "2 batches of epoch 20 completed.  Last Training Loss:  0.859064\n",
      "12 batches of epoch 20 completed.  Last Training Loss:  0.765608\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  1.008868\n",
      "32 batches of epoch 20 completed.  Last Training Loss:  0.763916\n",
      "42 batches of epoch 20 completed.  Last Training Loss:  0.985599\n",
      "52 batches of epoch 20 completed.  Last Training Loss:  0.791883\n",
      "Last Validation Loss:  12.514321, Lowest Validation Loss:  9.903163\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunFour.pt')\n",
    "# Fifth training run.  Got training loss under one, so expect conditioned to make reasonable one-token\n",
    "# forward inference.  Turn off teaching.\n",
    "\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset,validation_dataset, \\\n",
    "            ses_lrn = 0.0005, ses_tea = 0.0, ses_epochs = 20, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 batches of epoch 1 completed.  Last Training Loss:  9.115084\n",
      "20 batches of epoch 1 completed.  Last Training Loss:  7.586810\n",
      "30 batches of epoch 1 completed.  Last Training Loss:  7.292883\n",
      "40 batches of epoch 1 completed.  Last Training Loss:  6.997441\n",
      "50 batches of epoch 1 completed.  Last Training Loss:  6.683289\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.563903, Lowest Validation Loss:  7.563903\n",
      "8 batches of epoch 2 completed.  Last Training Loss:  6.366477\n",
      "18 batches of epoch 2 completed.  Last Training Loss:  6.256734\n",
      "28 batches of epoch 2 completed.  Last Training Loss:  6.255616\n",
      "38 batches of epoch 2 completed.  Last Training Loss:  5.929092\n",
      "48 batches of epoch 2 completed.  Last Training Loss:  5.870816\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.369114, Lowest Validation Loss:  7.369114\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  5.714532\n",
      "16 batches of epoch 3 completed.  Last Training Loss:  5.622810\n",
      "26 batches of epoch 3 completed.  Last Training Loss:  5.622499\n",
      "36 batches of epoch 3 completed.  Last Training Loss:  5.713586\n",
      "46 batches of epoch 3 completed.  Last Training Loss:  5.690499\n",
      "Last Validation Loss:  7.697530, Lowest Validation Loss:  7.369114\n",
      "4 batches of epoch 4 completed.  Last Training Loss:  5.363849\n",
      "14 batches of epoch 4 completed.  Last Training Loss:  5.552993\n",
      "24 batches of epoch 4 completed.  Last Training Loss:  5.391679\n",
      "34 batches of epoch 4 completed.  Last Training Loss:  5.266180\n",
      "44 batches of epoch 4 completed.  Last Training Loss:  5.180231\n",
      "Last Validation Loss:  7.733190, Lowest Validation Loss:  7.369114\n",
      "2 batches of epoch 5 completed.  Last Training Loss:  5.405604\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  5.280280\n",
      "22 batches of epoch 5 completed.  Last Training Loss:  5.228696\n",
      "32 batches of epoch 5 completed.  Last Training Loss:  5.603471\n",
      "42 batches of epoch 5 completed.  Last Training Loss:  5.512593\n",
      "52 batches of epoch 5 completed.  Last Training Loss:  5.481063\n",
      "Last Validation Loss:  7.460425, Lowest Validation Loss:  7.369114\n",
      "10 batches of epoch 6 completed.  Last Training Loss:  4.889162\n",
      "20 batches of epoch 6 completed.  Last Training Loss:  5.150033\n",
      "30 batches of epoch 6 completed.  Last Training Loss:  5.118238\n",
      "40 batches of epoch 6 completed.  Last Training Loss:  5.146484\n",
      "50 batches of epoch 6 completed.  Last Training Loss:  5.487026\n",
      "Last Validation Loss:  7.788923, Lowest Validation Loss:  7.369114\n",
      "8 batches of epoch 7 completed.  Last Training Loss:  4.879394\n",
      "18 batches of epoch 7 completed.  Last Training Loss:  4.746027\n",
      "28 batches of epoch 7 completed.  Last Training Loss:  4.904722\n",
      "38 batches of epoch 7 completed.  Last Training Loss:  5.067206\n",
      "48 batches of epoch 7 completed.  Last Training Loss:  4.987986\n",
      "Last Validation Loss:  8.053780, Lowest Validation Loss:  7.369114\n",
      "6 batches of epoch 8 completed.  Last Training Loss:  4.885910\n",
      "16 batches of epoch 8 completed.  Last Training Loss:  4.414920\n",
      "26 batches of epoch 8 completed.  Last Training Loss:  4.964944\n",
      "36 batches of epoch 8 completed.  Last Training Loss:  4.800861\n",
      "46 batches of epoch 8 completed.  Last Training Loss:  4.629332\n",
      "Last Validation Loss:  7.923666, Lowest Validation Loss:  7.369114\n",
      "4 batches of epoch 9 completed.  Last Training Loss:  4.622886\n",
      "14 batches of epoch 9 completed.  Last Training Loss:  4.842650\n",
      "24 batches of epoch 9 completed.  Last Training Loss:  4.838144\n",
      "34 batches of epoch 9 completed.  Last Training Loss:  4.674325\n",
      "44 batches of epoch 9 completed.  Last Training Loss:  4.518211\n",
      "Last Validation Loss:  8.264904, Lowest Validation Loss:  7.369114\n",
      "2 batches of epoch 10 completed.  Last Training Loss:  4.467024\n",
      "12 batches of epoch 10 completed.  Last Training Loss:  4.363272\n",
      "22 batches of epoch 10 completed.  Last Training Loss:  4.251384\n",
      "32 batches of epoch 10 completed.  Last Training Loss:  4.433472\n",
      "42 batches of epoch 10 completed.  Last Training Loss:  4.346569\n",
      "52 batches of epoch 10 completed.  Last Training Loss:  4.482813\n",
      "Last Validation Loss:  8.482559, Lowest Validation Loss:  7.369114\n",
      "10 batches of epoch 11 completed.  Last Training Loss:  4.406303\n",
      "20 batches of epoch 11 completed.  Last Training Loss:  4.250996\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  4.319292\n",
      "40 batches of epoch 11 completed.  Last Training Loss:  4.005840\n",
      "50 batches of epoch 11 completed.  Last Training Loss:  3.904519\n",
      "Last Validation Loss:  8.448264, Lowest Validation Loss:  7.369114\n",
      "8 batches of epoch 12 completed.  Last Training Loss:  4.033367\n",
      "18 batches of epoch 12 completed.  Last Training Loss:  3.881793\n",
      "28 batches of epoch 12 completed.  Last Training Loss:  4.473794\n",
      "38 batches of epoch 12 completed.  Last Training Loss:  4.084537\n",
      "48 batches of epoch 12 completed.  Last Training Loss:  4.172077\n",
      "Last Validation Loss:  8.834952, Lowest Validation Loss:  7.369114\n",
      "6 batches of epoch 13 completed.  Last Training Loss:  3.694272\n",
      "16 batches of epoch 13 completed.  Last Training Loss:  3.599153\n",
      "26 batches of epoch 13 completed.  Last Training Loss:  3.725692\n",
      "36 batches of epoch 13 completed.  Last Training Loss:  4.225983\n",
      "46 batches of epoch 13 completed.  Last Training Loss:  4.149373\n",
      "Last Validation Loss:  8.910184, Lowest Validation Loss:  7.369114\n",
      "4 batches of epoch 14 completed.  Last Training Loss:  3.555300\n",
      "14 batches of epoch 14 completed.  Last Training Loss:  3.475621\n",
      "24 batches of epoch 14 completed.  Last Training Loss:  3.191480\n",
      "34 batches of epoch 14 completed.  Last Training Loss:  3.635419\n",
      "44 batches of epoch 14 completed.  Last Training Loss:  3.646496\n",
      "Last Validation Loss:  8.870491, Lowest Validation Loss:  7.369114\n",
      "2 batches of epoch 15 completed.  Last Training Loss:  3.114886\n",
      "12 batches of epoch 15 completed.  Last Training Loss:  3.448282\n",
      "22 batches of epoch 15 completed.  Last Training Loss:  3.726467\n",
      "32 batches of epoch 15 completed.  Last Training Loss:  3.228055\n",
      "42 batches of epoch 15 completed.  Last Training Loss:  3.277548\n",
      "52 batches of epoch 15 completed.  Last Training Loss:  3.342393\n",
      "Last Validation Loss:  8.815975, Lowest Validation Loss:  7.369114\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  3.251042\n",
      "20 batches of epoch 16 completed.  Last Training Loss:  3.239832\n",
      "30 batches of epoch 16 completed.  Last Training Loss:  2.922440\n",
      "40 batches of epoch 16 completed.  Last Training Loss:  2.933497\n",
      "50 batches of epoch 16 completed.  Last Training Loss:  3.428598\n",
      "Last Validation Loss:  9.088124, Lowest Validation Loss:  7.369114\n",
      "8 batches of epoch 17 completed.  Last Training Loss:  2.779876\n",
      "18 batches of epoch 17 completed.  Last Training Loss:  2.887263\n",
      "28 batches of epoch 17 completed.  Last Training Loss:  3.094720\n",
      "38 batches of epoch 17 completed.  Last Training Loss:  2.588589\n",
      "48 batches of epoch 17 completed.  Last Training Loss:  3.410692\n",
      "Last Validation Loss:  9.734013, Lowest Validation Loss:  7.369114\n",
      "6 batches of epoch 18 completed.  Last Training Loss:  2.834142\n",
      "16 batches of epoch 18 completed.  Last Training Loss:  2.478756\n",
      "26 batches of epoch 18 completed.  Last Training Loss:  2.602560\n",
      "36 batches of epoch 18 completed.  Last Training Loss:  2.522545\n",
      "46 batches of epoch 18 completed.  Last Training Loss:  2.664922\n",
      "Last Validation Loss:  9.785712, Lowest Validation Loss:  7.369114\n",
      "4 batches of epoch 19 completed.  Last Training Loss:  2.821491\n",
      "14 batches of epoch 19 completed.  Last Training Loss:  2.786223\n",
      "24 batches of epoch 19 completed.  Last Training Loss:  2.464831\n",
      "34 batches of epoch 19 completed.  Last Training Loss:  2.497001\n",
      "44 batches of epoch 19 completed.  Last Training Loss:  2.314418\n",
      "Last Validation Loss:  9.475533, Lowest Validation Loss:  7.369114\n",
      "2 batches of epoch 20 completed.  Last Training Loss:  1.993020\n",
      "12 batches of epoch 20 completed.  Last Training Loss:  2.524747\n",
      "22 batches of epoch 20 completed.  Last Training Loss:  1.800984\n",
      "32 batches of epoch 20 completed.  Last Training Loss:  1.949146\n",
      "42 batches of epoch 20 completed.  Last Training Loss:  2.220211\n",
      "52 batches of epoch 20 completed.  Last Training Loss:  2.308605\n",
      "Last Validation Loss:  9.358180, Lowest Validation Loss:  7.369114\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunFive.pt')\n",
    "# Sixth training run.  Seems to not be generalizing, so giving it a new batch of 5000 training examples on which\n",
    "# to train without teaching\n",
    "train_dataset_2 = qaWithContextDataset(train_qa_df[5000:10000])\n",
    "\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset_2,validation_dataset, \\\n",
    "            ses_lrn = 0.0005, ses_tea = 0.0, ses_epochs = 20, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many questions available: 59317\n",
      "10 batches of epoch 1 completed.  Last Training Loss:  6.715934\n",
      "20 batches of epoch 1 completed.  Last Training Loss:  6.702463\n",
      "30 batches of epoch 1 completed.  Last Training Loss:  6.370927\n",
      "40 batches of epoch 1 completed.  Last Training Loss:  6.442051\n",
      "50 batches of epoch 1 completed.  Last Training Loss:  6.280188\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.047170, Lowest Validation Loss:  7.047170\n",
      "60 batches of epoch 1 completed.  Last Training Loss:  6.330679\n",
      "70 batches of epoch 1 completed.  Last Training Loss:  6.321343\n",
      "80 batches of epoch 1 completed.  Last Training Loss:  6.341338\n",
      "90 batches of epoch 1 completed.  Last Training Loss:  5.837990\n",
      "100 batches of epoch 1 completed.  Last Training Loss:  5.876680\n",
      "Saving model . . .\n",
      "Last Validation Loss:  6.840302, Lowest Validation Loss:  6.840302\n",
      "6 batches of epoch 2 completed.  Last Training Loss:  5.641878\n",
      "16 batches of epoch 2 completed.  Last Training Loss:  5.732736\n",
      "26 batches of epoch 2 completed.  Last Training Loss:  5.919466\n",
      "36 batches of epoch 2 completed.  Last Training Loss:  5.709380\n",
      "46 batches of epoch 2 completed.  Last Training Loss:  5.651138\n",
      "Saving model . . .\n",
      "Last Validation Loss:  6.652930, Lowest Validation Loss:  6.652930\n",
      "56 batches of epoch 2 completed.  Last Training Loss:  5.922093\n",
      "66 batches of epoch 2 completed.  Last Training Loss:  5.780329\n",
      "76 batches of epoch 2 completed.  Last Training Loss:  5.316597\n",
      "86 batches of epoch 2 completed.  Last Training Loss:  5.715437\n",
      "96 batches of epoch 2 completed.  Last Training Loss:  5.748789\n",
      "Last Validation Loss:  6.870510, Lowest Validation Loss:  6.652930\n",
      "2 batches of epoch 3 completed.  Last Training Loss:  5.351943\n",
      "12 batches of epoch 3 completed.  Last Training Loss:  5.145102\n",
      "22 batches of epoch 3 completed.  Last Training Loss:  5.681135\n",
      "32 batches of epoch 3 completed.  Last Training Loss:  5.456927\n",
      "42 batches of epoch 3 completed.  Last Training Loss:  5.270324\n",
      "52 batches of epoch 3 completed.  Last Training Loss:  5.548693\n",
      "Last Validation Loss:  7.435509, Lowest Validation Loss:  6.652930\n",
      "62 batches of epoch 3 completed.  Last Training Loss:  5.311906\n",
      "72 batches of epoch 3 completed.  Last Training Loss:  5.271144\n",
      "82 batches of epoch 3 completed.  Last Training Loss:  5.251111\n",
      "92 batches of epoch 3 completed.  Last Training Loss:  5.289226\n",
      "102 batches of epoch 3 completed.  Last Training Loss:  5.316127\n",
      "Last Validation Loss:  6.960601, Lowest Validation Loss:  6.652930\n",
      "8 batches of epoch 4 completed.  Last Training Loss:  5.162561\n",
      "18 batches of epoch 4 completed.  Last Training Loss:  5.135567\n",
      "28 batches of epoch 4 completed.  Last Training Loss:  5.157758\n",
      "38 batches of epoch 4 completed.  Last Training Loss:  5.495489\n",
      "48 batches of epoch 4 completed.  Last Training Loss:  5.096570\n",
      "Last Validation Loss:  6.941651, Lowest Validation Loss:  6.652930\n",
      "58 batches of epoch 4 completed.  Last Training Loss:  5.015430\n",
      "68 batches of epoch 4 completed.  Last Training Loss:  5.336778\n",
      "78 batches of epoch 4 completed.  Last Training Loss:  5.305950\n",
      "88 batches of epoch 4 completed.  Last Training Loss:  5.137690\n",
      "98 batches of epoch 4 completed.  Last Training Loss:  5.116276\n",
      "Last Validation Loss:  7.048613, Lowest Validation Loss:  6.652930\n",
      "4 batches of epoch 5 completed.  Last Training Loss:  4.896478\n",
      "14 batches of epoch 5 completed.  Last Training Loss:  5.030637\n",
      "24 batches of epoch 5 completed.  Last Training Loss:  5.022759\n",
      "34 batches of epoch 5 completed.  Last Training Loss:  5.132983\n",
      "44 batches of epoch 5 completed.  Last Training Loss:  4.951336\n",
      "Last Validation Loss:  7.599317, Lowest Validation Loss:  6.652930\n",
      "54 batches of epoch 5 completed.  Last Training Loss:  5.169849\n",
      "64 batches of epoch 5 completed.  Last Training Loss:  5.006082\n",
      "74 batches of epoch 5 completed.  Last Training Loss:  4.984429\n",
      "84 batches of epoch 5 completed.  Last Training Loss:  4.936881\n",
      "94 batches of epoch 5 completed.  Last Training Loss:  4.980082\n",
      "104 batches of epoch 5 completed.  Last Training Loss:  4.971641\n",
      "Last Validation Loss:  7.294057, Lowest Validation Loss:  6.652930\n",
      "10 batches of epoch 6 completed.  Last Training Loss:  4.737080\n",
      "20 batches of epoch 6 completed.  Last Training Loss:  4.519053\n",
      "30 batches of epoch 6 completed.  Last Training Loss:  4.948241\n",
      "40 batches of epoch 6 completed.  Last Training Loss:  4.820447\n",
      "50 batches of epoch 6 completed.  Last Training Loss:  4.508300\n",
      "Last Validation Loss:  7.625906, Lowest Validation Loss:  6.652930\n",
      "60 batches of epoch 6 completed.  Last Training Loss:  4.779076\n",
      "70 batches of epoch 6 completed.  Last Training Loss:  4.643822\n",
      "80 batches of epoch 6 completed.  Last Training Loss:  4.698877\n",
      "90 batches of epoch 6 completed.  Last Training Loss:  4.747983\n",
      "100 batches of epoch 6 completed.  Last Training Loss:  4.676605\n",
      "Last Validation Loss:  7.987156, Lowest Validation Loss:  6.652930\n",
      "6 batches of epoch 7 completed.  Last Training Loss:  4.546711\n",
      "16 batches of epoch 7 completed.  Last Training Loss:  4.549313\n",
      "26 batches of epoch 7 completed.  Last Training Loss:  4.455820\n",
      "36 batches of epoch 7 completed.  Last Training Loss:  4.270097\n",
      "46 batches of epoch 7 completed.  Last Training Loss:  4.252286\n",
      "Last Validation Loss:  8.214956, Lowest Validation Loss:  6.652930\n",
      "56 batches of epoch 7 completed.  Last Training Loss:  4.298326\n",
      "66 batches of epoch 7 completed.  Last Training Loss:  4.500606\n",
      "76 batches of epoch 7 completed.  Last Training Loss:  4.753922\n",
      "86 batches of epoch 7 completed.  Last Training Loss:  4.671649\n",
      "96 batches of epoch 7 completed.  Last Training Loss:  4.429374\n",
      "Last Validation Loss:  7.802077, Lowest Validation Loss:  6.652930\n",
      "2 batches of epoch 8 completed.  Last Training Loss:  4.357025\n",
      "12 batches of epoch 8 completed.  Last Training Loss:  4.420780\n",
      "22 batches of epoch 8 completed.  Last Training Loss:  3.992986\n",
      "32 batches of epoch 8 completed.  Last Training Loss:  3.905529\n",
      "42 batches of epoch 8 completed.  Last Training Loss:  4.451426\n",
      "52 batches of epoch 8 completed.  Last Training Loss:  4.027722\n",
      "Last Validation Loss:  7.788239, Lowest Validation Loss:  6.652930\n",
      "62 batches of epoch 8 completed.  Last Training Loss:  4.023060\n",
      "72 batches of epoch 8 completed.  Last Training Loss:  4.289392\n",
      "82 batches of epoch 8 completed.  Last Training Loss:  4.175421\n",
      "92 batches of epoch 8 completed.  Last Training Loss:  3.963239\n",
      "102 batches of epoch 8 completed.  Last Training Loss:  4.273148\n",
      "Last Validation Loss:  8.029075, Lowest Validation Loss:  6.652930\n",
      "8 batches of epoch 9 completed.  Last Training Loss:  3.748247\n",
      "18 batches of epoch 9 completed.  Last Training Loss:  3.947706\n",
      "28 batches of epoch 9 completed.  Last Training Loss:  3.897205\n",
      "38 batches of epoch 9 completed.  Last Training Loss:  3.739956\n",
      "48 batches of epoch 9 completed.  Last Training Loss:  3.751039\n",
      "Last Validation Loss:  8.239761, Lowest Validation Loss:  6.652930\n",
      "58 batches of epoch 9 completed.  Last Training Loss:  4.146581\n",
      "68 batches of epoch 9 completed.  Last Training Loss:  3.832742\n",
      "78 batches of epoch 9 completed.  Last Training Loss:  4.052890\n",
      "88 batches of epoch 9 completed.  Last Training Loss:  4.015937\n",
      "98 batches of epoch 9 completed.  Last Training Loss:  4.292244\n",
      "Last Validation Loss:  8.270962, Lowest Validation Loss:  6.652930\n",
      "4 batches of epoch 10 completed.  Last Training Loss:  3.611209\n",
      "14 batches of epoch 10 completed.  Last Training Loss:  3.617350\n",
      "24 batches of epoch 10 completed.  Last Training Loss:  3.469156\n",
      "34 batches of epoch 10 completed.  Last Training Loss:  3.719881\n",
      "44 batches of epoch 10 completed.  Last Training Loss:  3.466832\n",
      "Last Validation Loss:  8.404203, Lowest Validation Loss:  6.652930\n",
      "54 batches of epoch 10 completed.  Last Training Loss:  3.714158\n",
      "64 batches of epoch 10 completed.  Last Training Loss:  3.419291\n",
      "74 batches of epoch 10 completed.  Last Training Loss:  3.894144\n",
      "84 batches of epoch 10 completed.  Last Training Loss:  3.598621\n",
      "94 batches of epoch 10 completed.  Last Training Loss:  3.668281\n",
      "104 batches of epoch 10 completed.  Last Training Loss:  3.497464\n",
      "Last Validation Loss:  8.288434, Lowest Validation Loss:  6.652930\n",
      "10 batches of epoch 11 completed.  Last Training Loss:  3.207990\n",
      "20 batches of epoch 11 completed.  Last Training Loss:  3.512542\n",
      "30 batches of epoch 11 completed.  Last Training Loss:  3.181072\n",
      "40 batches of epoch 11 completed.  Last Training Loss:  2.962636\n",
      "50 batches of epoch 11 completed.  Last Training Loss:  3.702183\n",
      "Last Validation Loss:  8.204463, Lowest Validation Loss:  6.652930\n",
      "60 batches of epoch 11 completed.  Last Training Loss:  3.341993\n",
      "70 batches of epoch 11 completed.  Last Training Loss:  3.564323\n",
      "80 batches of epoch 11 completed.  Last Training Loss:  3.749430\n",
      "90 batches of epoch 11 completed.  Last Training Loss:  3.474517\n",
      "100 batches of epoch 11 completed.  Last Training Loss:  3.396400\n",
      "Last Validation Loss:  8.179851, Lowest Validation Loss:  6.652930\n",
      "6 batches of epoch 12 completed.  Last Training Loss:  3.348987\n",
      "16 batches of epoch 12 completed.  Last Training Loss:  2.889855\n",
      "26 batches of epoch 12 completed.  Last Training Loss:  3.284626\n",
      "36 batches of epoch 12 completed.  Last Training Loss:  3.147310\n",
      "46 batches of epoch 12 completed.  Last Training Loss:  3.110426\n",
      "Last Validation Loss:  9.245322, Lowest Validation Loss:  6.652930\n",
      "56 batches of epoch 12 completed.  Last Training Loss:  3.109748\n",
      "66 batches of epoch 12 completed.  Last Training Loss:  3.160346\n",
      "76 batches of epoch 12 completed.  Last Training Loss:  3.181201\n",
      "86 batches of epoch 12 completed.  Last Training Loss:  2.803673\n",
      "96 batches of epoch 12 completed.  Last Training Loss:  3.204097\n",
      "Last Validation Loss:  8.947585, Lowest Validation Loss:  6.652930\n",
      "2 batches of epoch 13 completed.  Last Training Loss:  2.258673\n",
      "12 batches of epoch 13 completed.  Last Training Loss:  2.922814\n",
      "22 batches of epoch 13 completed.  Last Training Loss:  2.432148\n",
      "32 batches of epoch 13 completed.  Last Training Loss:  2.591774\n",
      "42 batches of epoch 13 completed.  Last Training Loss:  2.783008\n",
      "52 batches of epoch 13 completed.  Last Training Loss:  2.790622\n",
      "Last Validation Loss:  9.764719, Lowest Validation Loss:  6.652930\n",
      "62 batches of epoch 13 completed.  Last Training Loss:  2.886262\n",
      "72 batches of epoch 13 completed.  Last Training Loss:  2.539027\n",
      "82 batches of epoch 13 completed.  Last Training Loss:  2.762044\n",
      "92 batches of epoch 13 completed.  Last Training Loss:  2.920152\n",
      "102 batches of epoch 13 completed.  Last Training Loss:  2.940894\n",
      "Last Validation Loss:  9.092647, Lowest Validation Loss:  6.652930\n",
      "8 batches of epoch 14 completed.  Last Training Loss:  2.565524\n",
      "18 batches of epoch 14 completed.  Last Training Loss:  2.548338\n",
      "28 batches of epoch 14 completed.  Last Training Loss:  2.651428\n",
      "38 batches of epoch 14 completed.  Last Training Loss:  2.653212\n",
      "48 batches of epoch 14 completed.  Last Training Loss:  2.369822\n",
      "Last Validation Loss:  9.462613, Lowest Validation Loss:  6.652930\n",
      "58 batches of epoch 14 completed.  Last Training Loss:  2.504048\n",
      "68 batches of epoch 14 completed.  Last Training Loss:  2.788443\n",
      "78 batches of epoch 14 completed.  Last Training Loss:  2.295952\n",
      "88 batches of epoch 14 completed.  Last Training Loss:  2.390638\n",
      "98 batches of epoch 14 completed.  Last Training Loss:  2.506005\n",
      "Last Validation Loss:  9.002674, Lowest Validation Loss:  6.652930\n",
      "4 batches of epoch 15 completed.  Last Training Loss:  2.248933\n",
      "14 batches of epoch 15 completed.  Last Training Loss:  2.342341\n",
      "24 batches of epoch 15 completed.  Last Training Loss:  1.977772\n",
      "34 batches of epoch 15 completed.  Last Training Loss:  1.982199\n",
      "44 batches of epoch 15 completed.  Last Training Loss:  2.353062\n",
      "Last Validation Loss:  9.396747, Lowest Validation Loss:  6.652930\n",
      "54 batches of epoch 15 completed.  Last Training Loss:  2.047790\n",
      "64 batches of epoch 15 completed.  Last Training Loss:  2.159501\n",
      "74 batches of epoch 15 completed.  Last Training Loss:  2.210388\n",
      "84 batches of epoch 15 completed.  Last Training Loss:  2.134833\n",
      "94 batches of epoch 15 completed.  Last Training Loss:  2.447546\n",
      "104 batches of epoch 15 completed.  Last Training Loss:  2.440298\n",
      "Last Validation Loss:  9.659239, Lowest Validation Loss:  6.652930\n",
      "10 batches of epoch 16 completed.  Last Training Loss:  1.999466\n",
      "20 batches of epoch 16 completed.  Last Training Loss:  1.915242\n",
      "30 batches of epoch 16 completed.  Last Training Loss:  2.039611\n",
      "40 batches of epoch 16 completed.  Last Training Loss:  2.062378\n",
      "50 batches of epoch 16 completed.  Last Training Loss:  1.970809\n",
      "Last Validation Loss:  10.126575, Lowest Validation Loss:  6.652930\n",
      "60 batches of epoch 16 completed.  Last Training Loss:  1.808515\n",
      "70 batches of epoch 16 completed.  Last Training Loss:  2.115690\n",
      "80 batches of epoch 16 completed.  Last Training Loss:  2.361469\n",
      "90 batches of epoch 16 completed.  Last Training Loss:  1.681686\n",
      "100 batches of epoch 16 completed.  Last Training Loss:  2.386656\n",
      "Last Validation Loss:  10.424135, Lowest Validation Loss:  6.652930\n",
      "6 batches of epoch 17 completed.  Last Training Loss:  1.689799\n",
      "16 batches of epoch 17 completed.  Last Training Loss:  1.795986\n",
      "26 batches of epoch 17 completed.  Last Training Loss:  1.880286\n",
      "36 batches of epoch 17 completed.  Last Training Loss:  1.778276\n",
      "46 batches of epoch 17 completed.  Last Training Loss:  1.663220\n",
      "Last Validation Loss:  9.786091, Lowest Validation Loss:  6.652930\n",
      "56 batches of epoch 17 completed.  Last Training Loss:  1.873923\n",
      "66 batches of epoch 17 completed.  Last Training Loss:  1.696811\n",
      "76 batches of epoch 17 completed.  Last Training Loss:  1.712786\n",
      "86 batches of epoch 17 completed.  Last Training Loss:  2.098052\n",
      "96 batches of epoch 17 completed.  Last Training Loss:  1.987697\n",
      "Last Validation Loss:  10.340781, Lowest Validation Loss:  6.652930\n",
      "2 batches of epoch 18 completed.  Last Training Loss:  1.490656\n",
      "12 batches of epoch 18 completed.  Last Training Loss:  1.635758\n",
      "22 batches of epoch 18 completed.  Last Training Loss:  1.575421\n",
      "32 batches of epoch 18 completed.  Last Training Loss:  1.771541\n",
      "42 batches of epoch 18 completed.  Last Training Loss:  1.660259\n",
      "52 batches of epoch 18 completed.  Last Training Loss:  1.838456\n",
      "Last Validation Loss:  9.758385, Lowest Validation Loss:  6.652930\n",
      "62 batches of epoch 18 completed.  Last Training Loss:  1.862183\n",
      "72 batches of epoch 18 completed.  Last Training Loss:  2.018314\n",
      "82 batches of epoch 18 completed.  Last Training Loss:  1.362001\n",
      "92 batches of epoch 18 completed.  Last Training Loss:  1.381032\n",
      "102 batches of epoch 18 completed.  Last Training Loss:  1.745539\n",
      "Last Validation Loss:  9.847940, Lowest Validation Loss:  6.652930\n",
      "8 batches of epoch 19 completed.  Last Training Loss:  1.761450\n",
      "18 batches of epoch 19 completed.  Last Training Loss:  1.518410\n",
      "28 batches of epoch 19 completed.  Last Training Loss:  1.560176\n",
      "38 batches of epoch 19 completed.  Last Training Loss:  1.547875\n",
      "48 batches of epoch 19 completed.  Last Training Loss:  1.211420\n",
      "Last Validation Loss:  10.183529, Lowest Validation Loss:  6.652930\n",
      "58 batches of epoch 19 completed.  Last Training Loss:  1.407881\n",
      "68 batches of epoch 19 completed.  Last Training Loss:  1.717602\n",
      "78 batches of epoch 19 completed.  Last Training Loss:  1.393612\n",
      "88 batches of epoch 19 completed.  Last Training Loss:  1.481072\n",
      "98 batches of epoch 19 completed.  Last Training Loss:  1.572792\n",
      "Last Validation Loss:  10.731907, Lowest Validation Loss:  6.652930\n",
      "4 batches of epoch 20 completed.  Last Training Loss:  1.195428\n",
      "14 batches of epoch 20 completed.  Last Training Loss:  1.314996\n",
      "24 batches of epoch 20 completed.  Last Training Loss:  1.292108\n",
      "34 batches of epoch 20 completed.  Last Training Loss:  1.127522\n",
      "44 batches of epoch 20 completed.  Last Training Loss:  1.037348\n",
      "Last Validation Loss:  10.033154, Lowest Validation Loss:  6.652930\n",
      "54 batches of epoch 20 completed.  Last Training Loss:  1.248017\n",
      "64 batches of epoch 20 completed.  Last Training Loss:  1.425749\n",
      "74 batches of epoch 20 completed.  Last Training Loss:  1.338238\n",
      "84 batches of epoch 20 completed.  Last Training Loss:  1.316600\n",
      "94 batches of epoch 20 completed.  Last Training Loss:  1.355384\n",
      "104 batches of epoch 20 completed.  Last Training Loss:  1.600941\n",
      "Last Validation Loss:  10.076414, Lowest Validation Loss:  6.652930\n"
     ]
    }
   ],
   "source": [
    "torch.save(seqToseqNet,'afterRunSix.pt')\n",
    "\n",
    "print(f\"How many questions available: {train_qa_df['qIdxs'].count()}\")\n",
    "# Seventh training run.  Some validation performance improvement but still tending to overfit.\n",
    "# Giving it a new batch of 10000 training examples on which\n",
    "# to train without teaching\n",
    "\n",
    "# Go back to lowest validation network due to overfitting on last run\n",
    "seqToseqNet = torch.load(\"Min-Validation-Loss-Model.pt\") \n",
    "\n",
    "train_dataset_3 = qaWithContextDataset(train_qa_df[10000:20000])\n",
    "\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset_3,validation_dataset, \\\n",
    "            ses_lrn = 0.0005, ses_tea = 0.0, ses_epochs = 20, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(seqToseqNet,'afterRunSeven.pt')\n",
    "\n",
    "# Go back to lowest validation network due to overfitting on last run\n",
    "# seqToseqNet = torch.load(\"Min-Validation-Loss-Model.pt\") \n",
    "\n",
    "# Run with all questions for fewer epochs to attempt best generalization\n",
    "train_dataset_4 = qaWithContextDataset(train_qa_df)\n",
    "\n",
    "# Fewer epochs since run will be longer\n",
    "# train_model(seqToseqNet, \\\n",
    "#            train_dataset_4,validation_dataset, \\\n",
    "#            ses_lrn = 0.0005, ses_tea = 0.0, ses_epochs = 10, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 batches of epoch 1 completed.  Last Training Loss:  9.761504\n",
      "20 batches of epoch 1 completed.  Last Training Loss:  7.624062\n",
      "30 batches of epoch 1 completed.  Last Training Loss:  8.439412\n",
      "40 batches of epoch 1 completed.  Last Training Loss:  6.849150\n",
      "50 batches of epoch 1 completed.  Last Training Loss:  8.349634\n",
      "Saving model . . .\n",
      "Last Validation Loss:  8.515495, Lowest Validation Loss:  8.515495\n",
      "60 batches of epoch 1 completed.  Last Training Loss:  6.973281\n",
      "70 batches of epoch 1 completed.  Last Training Loss:  6.728694\n",
      "80 batches of epoch 1 completed.  Last Training Loss:  7.642511\n",
      "90 batches of epoch 1 completed.  Last Training Loss:  7.599713\n",
      "100 batches of epoch 1 completed.  Last Training Loss:  7.068876\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.582142, Lowest Validation Loss:  7.582142\n",
      "110 batches of epoch 1 completed.  Last Training Loss:  6.967543\n",
      "120 batches of epoch 1 completed.  Last Training Loss:  7.328267\n",
      "130 batches of epoch 1 completed.  Last Training Loss:  6.677456\n",
      "140 batches of epoch 1 completed.  Last Training Loss:  7.204705\n",
      "150 batches of epoch 1 completed.  Last Training Loss:  7.091914\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.343968, Lowest Validation Loss:  7.343968\n",
      "160 batches of epoch 1 completed.  Last Training Loss:  6.707684\n",
      "170 batches of epoch 1 completed.  Last Training Loss:  6.708537\n",
      "180 batches of epoch 1 completed.  Last Training Loss:  6.553020\n",
      "190 batches of epoch 1 completed.  Last Training Loss:  6.592218\n",
      "200 batches of epoch 1 completed.  Last Training Loss:  7.022583\n",
      "Last Validation Loss:  7.424509, Lowest Validation Loss:  7.343968\n",
      "210 batches of epoch 1 completed.  Last Training Loss:  6.967978\n",
      "220 batches of epoch 1 completed.  Last Training Loss:  6.195563\n",
      "230 batches of epoch 1 completed.  Last Training Loss:  6.619702\n",
      "240 batches of epoch 1 completed.  Last Training Loss:  7.316693\n",
      "250 batches of epoch 1 completed.  Last Training Loss:  6.979779\n",
      "260 batches of epoch 1 completed.  Last Training Loss:  6.998299\n",
      "Last Validation Loss:  7.541122, Lowest Validation Loss:  7.343968\n",
      "270 batches of epoch 1 completed.  Last Training Loss:  6.554725\n",
      "280 batches of epoch 1 completed.  Last Training Loss:  7.510054\n",
      "290 batches of epoch 1 completed.  Last Training Loss:  6.450040\n",
      "300 batches of epoch 1 completed.  Last Training Loss:  6.928567\n",
      "310 batches of epoch 1 completed.  Last Training Loss:  6.600294\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.343951, Lowest Validation Loss:  7.343951\n",
      "320 batches of epoch 1 completed.  Last Training Loss:  7.058919\n",
      "330 batches of epoch 1 completed.  Last Training Loss:  6.863865\n",
      "340 batches of epoch 1 completed.  Last Training Loss:  6.847321\n",
      "350 batches of epoch 1 completed.  Last Training Loss:  6.589307\n",
      "360 batches of epoch 1 completed.  Last Training Loss:  6.606383\n",
      "Saving model . . .\n",
      "Last Validation Loss:  7.004725, Lowest Validation Loss:  7.004725\n",
      "370 batches of epoch 1 completed.  Last Training Loss:  6.752210\n",
      "380 batches of epoch 1 completed.  Last Training Loss:  6.884403\n",
      "390 batches of epoch 1 completed.  Last Training Loss:  6.878769\n",
      "400 batches of epoch 1 completed.  Last Training Loss:  6.698316\n",
      "410 batches of epoch 1 completed.  Last Training Loss:  6.305177\n",
      "Last Validation Loss:  7.559442, Lowest Validation Loss:  7.004725\n",
      "420 batches of epoch 1 completed.  Last Training Loss:  6.734349\n",
      "430 batches of epoch 1 completed.  Last Training Loss:  6.949268\n",
      "440 batches of epoch 1 completed.  Last Training Loss:  6.753666\n",
      "450 batches of epoch 1 completed.  Last Training Loss:  6.509079\n",
      "460 batches of epoch 1 completed.  Last Training Loss:  7.156194\n",
      "Last Validation Loss:  7.021276, Lowest Validation Loss:  7.004725\n",
      "470 batches of epoch 1 completed.  Last Training Loss:  6.899187\n",
      "480 batches of epoch 1 completed.  Last Training Loss:  6.521195\n",
      "490 batches of epoch 1 completed.  Last Training Loss:  6.830056\n",
      "500 batches of epoch 1 completed.  Last Training Loss:  6.609030\n",
      "510 batches of epoch 1 completed.  Last Training Loss:  6.091992\n",
      "520 batches of epoch 1 completed.  Last Training Loss:  6.772015\n",
      "Saving model . . .\n",
      "Last Validation Loss:  6.847796, Lowest Validation Loss:  6.847796\n",
      "530 batches of epoch 1 completed.  Last Training Loss:  6.806579\n",
      "540 batches of epoch 1 completed.  Last Training Loss:  6.501507\n",
      "550 batches of epoch 1 completed.  Last Training Loss:  6.803277\n",
      "560 batches of epoch 1 completed.  Last Training Loss:  6.742552\n",
      "570 batches of epoch 1 completed.  Last Training Loss:  6.160967\n",
      "Last Validation Loss:  7.116939, Lowest Validation Loss:  6.847796\n",
      "580 batches of epoch 1 completed.  Last Training Loss:  6.924810\n",
      "590 batches of epoch 1 completed.  Last Training Loss:  6.354069\n",
      "600 batches of epoch 1 completed.  Last Training Loss:  6.898408\n",
      "610 batches of epoch 1 completed.  Last Training Loss:  6.412052\n",
      "3 batches of epoch 2 completed.  Last Training Loss:  6.592882\n",
      "Last Validation Loss:  6.963128, Lowest Validation Loss:  6.847796\n",
      "13 batches of epoch 2 completed.  Last Training Loss:  5.878954\n",
      "23 batches of epoch 2 completed.  Last Training Loss:  5.812484\n",
      "33 batches of epoch 2 completed.  Last Training Loss:  6.581627\n",
      "43 batches of epoch 2 completed.  Last Training Loss:  5.967451\n",
      "53 batches of epoch 2 completed.  Last Training Loss:  6.431721\n",
      "Last Validation Loss:  7.239627, Lowest Validation Loss:  6.847796\n",
      "63 batches of epoch 2 completed.  Last Training Loss:  6.395627\n",
      "73 batches of epoch 2 completed.  Last Training Loss:  6.665823\n",
      "83 batches of epoch 2 completed.  Last Training Loss:  6.579028\n",
      "93 batches of epoch 2 completed.  Last Training Loss:  6.809697\n",
      "103 batches of epoch 2 completed.  Last Training Loss:  6.630380\n",
      "Last Validation Loss:  7.123384, Lowest Validation Loss:  6.847796\n",
      "113 batches of epoch 2 completed.  Last Training Loss:  6.754320\n",
      "123 batches of epoch 2 completed.  Last Training Loss:  6.786193\n",
      "133 batches of epoch 2 completed.  Last Training Loss:  6.691895\n",
      "143 batches of epoch 2 completed.  Last Training Loss:  6.626397\n",
      "153 batches of epoch 2 completed.  Last Training Loss:  6.544552\n",
      "163 batches of epoch 2 completed.  Last Training Loss:  6.869625\n",
      "Last Validation Loss:  7.217699, Lowest Validation Loss:  6.847796\n",
      "173 batches of epoch 2 completed.  Last Training Loss:  6.565301\n",
      "183 batches of epoch 2 completed.  Last Training Loss:  6.619157\n",
      "193 batches of epoch 2 completed.  Last Training Loss:  6.815325\n",
      "203 batches of epoch 2 completed.  Last Training Loss:  6.625918\n",
      "213 batches of epoch 2 completed.  Last Training Loss:  6.568051\n",
      "Last Validation Loss:  7.212511, Lowest Validation Loss:  6.847796\n",
      "223 batches of epoch 2 completed.  Last Training Loss:  5.967672\n",
      "233 batches of epoch 2 completed.  Last Training Loss:  6.907771\n",
      "243 batches of epoch 2 completed.  Last Training Loss:  5.978136\n",
      "253 batches of epoch 2 completed.  Last Training Loss:  5.915139\n",
      "263 batches of epoch 2 completed.  Last Training Loss:  6.461217\n",
      "Last Validation Loss:  7.186444, Lowest Validation Loss:  6.847796\n",
      "273 batches of epoch 2 completed.  Last Training Loss:  6.675208\n",
      "283 batches of epoch 2 completed.  Last Training Loss:  6.599750\n",
      "293 batches of epoch 2 completed.  Last Training Loss:  6.781868\n",
      "303 batches of epoch 2 completed.  Last Training Loss:  5.787689\n",
      "313 batches of epoch 2 completed.  Last Training Loss:  5.801978\n",
      "Last Validation Loss:  7.081745, Lowest Validation Loss:  6.847796\n",
      "323 batches of epoch 2 completed.  Last Training Loss:  5.776837\n",
      "333 batches of epoch 2 completed.  Last Training Loss:  5.727242\n",
      "343 batches of epoch 2 completed.  Last Training Loss:  6.063562\n",
      "353 batches of epoch 2 completed.  Last Training Loss:  6.846527\n",
      "363 batches of epoch 2 completed.  Last Training Loss:  5.779658\n",
      "Last Validation Loss:  7.513175, Lowest Validation Loss:  6.847796\n",
      "373 batches of epoch 2 completed.  Last Training Loss:  6.776173\n",
      "383 batches of epoch 2 completed.  Last Training Loss:  6.829908\n",
      "393 batches of epoch 2 completed.  Last Training Loss:  6.648657\n",
      "403 batches of epoch 2 completed.  Last Training Loss:  6.140937\n",
      "413 batches of epoch 2 completed.  Last Training Loss:  5.700894\n",
      "423 batches of epoch 2 completed.  Last Training Loss:  6.231511\n",
      "Last Validation Loss:  7.113061, Lowest Validation Loss:  6.847796\n",
      "433 batches of epoch 2 completed.  Last Training Loss:  5.930509\n",
      "443 batches of epoch 2 completed.  Last Training Loss:  6.831004\n",
      "453 batches of epoch 2 completed.  Last Training Loss:  6.800074\n",
      "463 batches of epoch 2 completed.  Last Training Loss:  6.750590\n",
      "473 batches of epoch 2 completed.  Last Training Loss:  5.940501\n",
      "Last Validation Loss:  7.184256, Lowest Validation Loss:  6.847796\n",
      "483 batches of epoch 2 completed.  Last Training Loss:  5.896088\n",
      "493 batches of epoch 2 completed.  Last Training Loss:  7.070291\n",
      "503 batches of epoch 2 completed.  Last Training Loss:  5.873403\n",
      "513 batches of epoch 2 completed.  Last Training Loss:  6.747618\n",
      "523 batches of epoch 2 completed.  Last Training Loss:  5.516459\n",
      "Last Validation Loss:  7.076895, Lowest Validation Loss:  6.847796\n",
      "533 batches of epoch 2 completed.  Last Training Loss:  7.012256\n",
      "543 batches of epoch 2 completed.  Last Training Loss:  6.137457\n",
      "553 batches of epoch 2 completed.  Last Training Loss:  6.570867\n",
      "563 batches of epoch 2 completed.  Last Training Loss:  5.756630\n",
      "573 batches of epoch 2 completed.  Last Training Loss:  6.270295\n",
      "Last Validation Loss:  6.903888, Lowest Validation Loss:  6.847796\n",
      "583 batches of epoch 2 completed.  Last Training Loss:  6.265853\n",
      "593 batches of epoch 2 completed.  Last Training Loss:  5.822209\n",
      "603 batches of epoch 2 completed.  Last Training Loss:  6.711617\n",
      "613 batches of epoch 2 completed.  Last Training Loss:  5.728315\n",
      "6 batches of epoch 3 completed.  Last Training Loss:  5.079256\n",
      "Last Validation Loss:  7.194549, Lowest Validation Loss:  6.847796\n",
      "16 batches of epoch 3 completed.  Last Training Loss:  6.706275\n",
      "26 batches of epoch 3 completed.  Last Training Loss:  6.330899\n",
      "36 batches of epoch 3 completed.  Last Training Loss:  5.275739\n",
      "46 batches of epoch 3 completed.  Last Training Loss:  6.639727\n",
      "56 batches of epoch 3 completed.  Last Training Loss:  6.557065\n",
      "66 batches of epoch 3 completed.  Last Training Loss:  6.724608\n",
      "Last Validation Loss:  7.160490, Lowest Validation Loss:  6.847796\n",
      "76 batches of epoch 3 completed.  Last Training Loss:  6.686732\n",
      "86 batches of epoch 3 completed.  Last Training Loss:  6.575316\n",
      "96 batches of epoch 3 completed.  Last Training Loss:  6.511263\n",
      "106 batches of epoch 3 completed.  Last Training Loss:  5.483815\n",
      "116 batches of epoch 3 completed.  Last Training Loss:  6.843905\n",
      "Last Validation Loss:  7.289894, Lowest Validation Loss:  6.847796\n",
      "126 batches of epoch 3 completed.  Last Training Loss:  5.378925\n",
      "136 batches of epoch 3 completed.  Last Training Loss:  5.300484\n",
      "146 batches of epoch 3 completed.  Last Training Loss:  5.047296\n",
      "156 batches of epoch 3 completed.  Last Training Loss:  5.620789\n",
      "166 batches of epoch 3 completed.  Last Training Loss:  6.627771\n",
      "Last Validation Loss:  7.025716, Lowest Validation Loss:  6.847796\n",
      "176 batches of epoch 3 completed.  Last Training Loss:  6.420002\n",
      "186 batches of epoch 3 completed.  Last Training Loss:  6.462637\n",
      "196 batches of epoch 3 completed.  Last Training Loss:  5.354267\n",
      "206 batches of epoch 3 completed.  Last Training Loss:  5.357422\n",
      "216 batches of epoch 3 completed.  Last Training Loss:  5.426208\n",
      "Last Validation Loss:  7.488220, Lowest Validation Loss:  6.847796\n",
      "226 batches of epoch 3 completed.  Last Training Loss:  6.516422\n",
      "236 batches of epoch 3 completed.  Last Training Loss:  6.604115\n",
      "246 batches of epoch 3 completed.  Last Training Loss:  6.572882\n",
      "256 batches of epoch 3 completed.  Last Training Loss:  5.287824\n",
      "266 batches of epoch 3 completed.  Last Training Loss:  6.545634\n",
      "Last Validation Loss:  7.243478, Lowest Validation Loss:  6.847796\n",
      "276 batches of epoch 3 completed.  Last Training Loss:  6.721419\n",
      "286 batches of epoch 3 completed.  Last Training Loss:  6.655306\n",
      "296 batches of epoch 3 completed.  Last Training Loss:  5.527437\n",
      "306 batches of epoch 3 completed.  Last Training Loss:  6.708697\n",
      "316 batches of epoch 3 completed.  Last Training Loss:  5.759980\n",
      "326 batches of epoch 3 completed.  Last Training Loss:  6.545047\n",
      "Last Validation Loss:  7.257368, Lowest Validation Loss:  6.847796\n",
      "336 batches of epoch 3 completed.  Last Training Loss:  6.493412\n",
      "346 batches of epoch 3 completed.  Last Training Loss:  5.625062\n",
      "356 batches of epoch 3 completed.  Last Training Loss:  6.531396\n",
      "366 batches of epoch 3 completed.  Last Training Loss:  5.617948\n",
      "376 batches of epoch 3 completed.  Last Training Loss:  5.544082\n",
      "Last Validation Loss:  7.206801, Lowest Validation Loss:  6.847796\n",
      "386 batches of epoch 3 completed.  Last Training Loss:  6.477456\n",
      "396 batches of epoch 3 completed.  Last Training Loss:  5.356552\n",
      "406 batches of epoch 3 completed.  Last Training Loss:  5.345335\n",
      "416 batches of epoch 3 completed.  Last Training Loss:  5.443112\n",
      "426 batches of epoch 3 completed.  Last Training Loss:  5.947515\n",
      "Last Validation Loss:  7.248542, Lowest Validation Loss:  6.847796\n",
      "436 batches of epoch 3 completed.  Last Training Loss:  5.650910\n",
      "446 batches of epoch 3 completed.  Last Training Loss:  5.713942\n",
      "456 batches of epoch 3 completed.  Last Training Loss:  5.686900\n",
      "466 batches of epoch 3 completed.  Last Training Loss:  5.318161\n",
      "476 batches of epoch 3 completed.  Last Training Loss:  5.398763\n",
      "Last Validation Loss:  6.966064, Lowest Validation Loss:  6.847796\n",
      "486 batches of epoch 3 completed.  Last Training Loss:  6.580557\n",
      "496 batches of epoch 3 completed.  Last Training Loss:  6.707713\n",
      "506 batches of epoch 3 completed.  Last Training Loss:  7.007760\n",
      "516 batches of epoch 3 completed.  Last Training Loss:  5.638155\n",
      "526 batches of epoch 3 completed.  Last Training Loss:  5.763382\n",
      "Last Validation Loss:  7.410250, Lowest Validation Loss:  6.847796\n",
      "536 batches of epoch 3 completed.  Last Training Loss:  6.113523\n",
      "546 batches of epoch 3 completed.  Last Training Loss:  6.683394\n",
      "556 batches of epoch 3 completed.  Last Training Loss:  6.535290\n",
      "566 batches of epoch 3 completed.  Last Training Loss:  6.431643\n",
      "576 batches of epoch 3 completed.  Last Training Loss:  5.713809\n",
      "586 batches of epoch 3 completed.  Last Training Loss:  6.570298\n",
      "Last Validation Loss:  7.530029, Lowest Validation Loss:  6.847796\n",
      "596 batches of epoch 3 completed.  Last Training Loss:  5.438959\n",
      "606 batches of epoch 3 completed.  Last Training Loss:  5.568057\n",
      "616 batches of epoch 3 completed.  Last Training Loss:  5.798995\n",
      "9 batches of epoch 4 completed.  Last Training Loss:  4.617437\n",
      "19 batches of epoch 4 completed.  Last Training Loss:  5.050500\n",
      "Last Validation Loss:  7.396291, Lowest Validation Loss:  6.847796\n",
      "29 batches of epoch 4 completed.  Last Training Loss:  5.189395\n",
      "39 batches of epoch 4 completed.  Last Training Loss:  6.362288\n",
      "49 batches of epoch 4 completed.  Last Training Loss:  6.648730\n",
      "59 batches of epoch 4 completed.  Last Training Loss:  6.491629\n",
      "69 batches of epoch 4 completed.  Last Training Loss:  4.985736\n",
      "Last Validation Loss:  7.538281, Lowest Validation Loss:  6.847796\n",
      "79 batches of epoch 4 completed.  Last Training Loss:  5.087412\n",
      "89 batches of epoch 4 completed.  Last Training Loss:  6.317479\n",
      "99 batches of epoch 4 completed.  Last Training Loss:  5.164721\n",
      "109 batches of epoch 4 completed.  Last Training Loss:  6.728742\n",
      "119 batches of epoch 4 completed.  Last Training Loss:  4.924207\n",
      "Last Validation Loss:  7.381306, Lowest Validation Loss:  6.847796\n",
      "129 batches of epoch 4 completed.  Last Training Loss:  5.539219\n",
      "139 batches of epoch 4 completed.  Last Training Loss:  6.567233\n",
      "149 batches of epoch 4 completed.  Last Training Loss:  6.525336\n",
      "159 batches of epoch 4 completed.  Last Training Loss:  6.904344\n",
      "169 batches of epoch 4 completed.  Last Training Loss:  6.411645\n",
      "Last Validation Loss:  7.245908, Lowest Validation Loss:  6.847796\n",
      "179 batches of epoch 4 completed.  Last Training Loss:  5.026688\n",
      "189 batches of epoch 4 completed.  Last Training Loss:  6.387353\n",
      "199 batches of epoch 4 completed.  Last Training Loss:  5.311852\n",
      "209 batches of epoch 4 completed.  Last Training Loss:  5.326780\n",
      "219 batches of epoch 4 completed.  Last Training Loss:  5.126163\n",
      "229 batches of epoch 4 completed.  Last Training Loss:  6.274492\n",
      "Last Validation Loss:  7.334736, Lowest Validation Loss:  6.847796\n",
      "239 batches of epoch 4 completed.  Last Training Loss:  5.280643\n",
      "249 batches of epoch 4 completed.  Last Training Loss:  6.833842\n",
      "259 batches of epoch 4 completed.  Last Training Loss:  6.374647\n",
      "269 batches of epoch 4 completed.  Last Training Loss:  6.499334\n",
      "279 batches of epoch 4 completed.  Last Training Loss:  5.229027\n",
      "Last Validation Loss:  7.302455, Lowest Validation Loss:  6.847796\n",
      "289 batches of epoch 4 completed.  Last Training Loss:  6.524939\n",
      "299 batches of epoch 4 completed.  Last Training Loss:  5.164791\n",
      "309 batches of epoch 4 completed.  Last Training Loss:  6.822078\n",
      "319 batches of epoch 4 completed.  Last Training Loss:  6.694465\n",
      "329 batches of epoch 4 completed.  Last Training Loss:  6.731850\n",
      "Last Validation Loss:  7.165085, Lowest Validation Loss:  6.847796\n",
      "339 batches of epoch 4 completed.  Last Training Loss:  6.710353\n",
      "349 batches of epoch 4 completed.  Last Training Loss:  5.202409\n",
      "359 batches of epoch 4 completed.  Last Training Loss:  6.503047\n",
      "369 batches of epoch 4 completed.  Last Training Loss:  6.564472\n",
      "379 batches of epoch 4 completed.  Last Training Loss:  5.595282\n",
      "Last Validation Loss:  7.200055, Lowest Validation Loss:  6.847796\n",
      "389 batches of epoch 4 completed.  Last Training Loss:  6.642940\n",
      "399 batches of epoch 4 completed.  Last Training Loss:  6.434280\n",
      "409 batches of epoch 4 completed.  Last Training Loss:  5.456551\n",
      "419 batches of epoch 4 completed.  Last Training Loss:  5.287990\n",
      "429 batches of epoch 4 completed.  Last Training Loss:  5.419353\n",
      "Last Validation Loss:  7.616588, Lowest Validation Loss:  6.847796\n",
      "439 batches of epoch 4 completed.  Last Training Loss:  6.656595\n",
      "449 batches of epoch 4 completed.  Last Training Loss:  5.098045\n",
      "459 batches of epoch 4 completed.  Last Training Loss:  6.475544\n",
      "469 batches of epoch 4 completed.  Last Training Loss:  6.586473\n",
      "479 batches of epoch 4 completed.  Last Training Loss:  6.297282\n",
      "489 batches of epoch 4 completed.  Last Training Loss:  5.401073\n",
      "Last Validation Loss:  7.383010, Lowest Validation Loss:  6.847796\n",
      "499 batches of epoch 4 completed.  Last Training Loss:  5.091116\n",
      "509 batches of epoch 4 completed.  Last Training Loss:  6.579339\n",
      "519 batches of epoch 4 completed.  Last Training Loss:  4.912001\n",
      "529 batches of epoch 4 completed.  Last Training Loss:  5.203945\n",
      "539 batches of epoch 4 completed.  Last Training Loss:  5.195796\n",
      "Last Validation Loss:  7.146418, Lowest Validation Loss:  6.847796\n",
      "549 batches of epoch 4 completed.  Last Training Loss:  5.239150\n",
      "559 batches of epoch 4 completed.  Last Training Loss:  5.224904\n",
      "569 batches of epoch 4 completed.  Last Training Loss:  6.932589\n",
      "579 batches of epoch 4 completed.  Last Training Loss:  5.556290\n",
      "589 batches of epoch 4 completed.  Last Training Loss:  6.440648\n",
      "Last Validation Loss:  7.628657, Lowest Validation Loss:  6.847796\n",
      "599 batches of epoch 4 completed.  Last Training Loss:  5.535605\n",
      "609 batches of epoch 4 completed.  Last Training Loss:  5.512842\n",
      "2 batches of epoch 5 completed.  Last Training Loss:  6.431807\n",
      "12 batches of epoch 5 completed.  Last Training Loss:  4.513173\n",
      "22 batches of epoch 5 completed.  Last Training Loss:  6.559251\n",
      "Last Validation Loss:  7.513243, Lowest Validation Loss:  6.847796\n",
      "32 batches of epoch 5 completed.  Last Training Loss:  4.699019\n",
      "42 batches of epoch 5 completed.  Last Training Loss:  6.250978\n",
      "52 batches of epoch 5 completed.  Last Training Loss:  4.875995\n",
      "62 batches of epoch 5 completed.  Last Training Loss:  4.589664\n",
      "72 batches of epoch 5 completed.  Last Training Loss:  6.067785\n",
      "Last Validation Loss:  7.665182, Lowest Validation Loss:  6.847796\n",
      "82 batches of epoch 5 completed.  Last Training Loss:  4.963692\n",
      "92 batches of epoch 5 completed.  Last Training Loss:  4.749680\n",
      "102 batches of epoch 5 completed.  Last Training Loss:  6.359303\n",
      "112 batches of epoch 5 completed.  Last Training Loss:  6.557484\n",
      "122 batches of epoch 5 completed.  Last Training Loss:  4.662512\n",
      "132 batches of epoch 5 completed.  Last Training Loss:  4.682809\n",
      "Last Validation Loss:  7.360866, Lowest Validation Loss:  6.847796\n",
      "142 batches of epoch 5 completed.  Last Training Loss:  4.844531\n",
      "152 batches of epoch 5 completed.  Last Training Loss:  6.408781\n",
      "162 batches of epoch 5 completed.  Last Training Loss:  5.215960\n",
      "172 batches of epoch 5 completed.  Last Training Loss:  5.121314\n",
      "182 batches of epoch 5 completed.  Last Training Loss:  4.831810\n",
      "Last Validation Loss:  7.234365, Lowest Validation Loss:  6.847796\n",
      "192 batches of epoch 5 completed.  Last Training Loss:  6.285947\n",
      "202 batches of epoch 5 completed.  Last Training Loss:  5.018198\n",
      "212 batches of epoch 5 completed.  Last Training Loss:  6.599090\n",
      "222 batches of epoch 5 completed.  Last Training Loss:  5.511784\n",
      "232 batches of epoch 5 completed.  Last Training Loss:  4.975127\n",
      "Last Validation Loss:  7.673029, Lowest Validation Loss:  6.847796\n",
      "242 batches of epoch 5 completed.  Last Training Loss:  6.372812\n",
      "252 batches of epoch 5 completed.  Last Training Loss:  5.111317\n",
      "262 batches of epoch 5 completed.  Last Training Loss:  6.647142\n",
      "272 batches of epoch 5 completed.  Last Training Loss:  5.130906\n",
      "282 batches of epoch 5 completed.  Last Training Loss:  5.176613\n",
      "Last Validation Loss:  7.315697, Lowest Validation Loss:  6.847796\n",
      "292 batches of epoch 5 completed.  Last Training Loss:  5.190373\n",
      "302 batches of epoch 5 completed.  Last Training Loss:  5.154909\n",
      "312 batches of epoch 5 completed.  Last Training Loss:  6.692666\n",
      "322 batches of epoch 5 completed.  Last Training Loss:  4.841185\n",
      "332 batches of epoch 5 completed.  Last Training Loss:  5.086796\n",
      "Last Validation Loss:  7.561110, Lowest Validation Loss:  6.847796\n",
      "342 batches of epoch 5 completed.  Last Training Loss:  5.111582\n",
      "352 batches of epoch 5 completed.  Last Training Loss:  6.429861\n",
      "362 batches of epoch 5 completed.  Last Training Loss:  6.386951\n",
      "372 batches of epoch 5 completed.  Last Training Loss:  5.086524\n",
      "382 batches of epoch 5 completed.  Last Training Loss:  5.006544\n",
      "392 batches of epoch 5 completed.  Last Training Loss:  6.384349\n",
      "Last Validation Loss:  7.262869, Lowest Validation Loss:  6.847796\n",
      "402 batches of epoch 5 completed.  Last Training Loss:  6.580342\n",
      "412 batches of epoch 5 completed.  Last Training Loss:  6.493331\n",
      "422 batches of epoch 5 completed.  Last Training Loss:  4.906878\n",
      "432 batches of epoch 5 completed.  Last Training Loss:  5.097269\n",
      "442 batches of epoch 5 completed.  Last Training Loss:  5.373851\n",
      "Last Validation Loss:  7.482207, Lowest Validation Loss:  6.847796\n",
      "452 batches of epoch 5 completed.  Last Training Loss:  5.088102\n",
      "462 batches of epoch 5 completed.  Last Training Loss:  5.168834\n",
      "472 batches of epoch 5 completed.  Last Training Loss:  6.589706\n",
      "482 batches of epoch 5 completed.  Last Training Loss:  4.931945\n",
      "492 batches of epoch 5 completed.  Last Training Loss:  6.339255\n",
      "Last Validation Loss:  7.605320, Lowest Validation Loss:  6.847796\n",
      "502 batches of epoch 5 completed.  Last Training Loss:  5.385207\n",
      "512 batches of epoch 5 completed.  Last Training Loss:  5.064447\n",
      "522 batches of epoch 5 completed.  Last Training Loss:  5.051350\n",
      "532 batches of epoch 5 completed.  Last Training Loss:  6.719487\n",
      "542 batches of epoch 5 completed.  Last Training Loss:  5.187442\n",
      "Last Validation Loss:  7.380765, Lowest Validation Loss:  6.847796\n",
      "552 batches of epoch 5 completed.  Last Training Loss:  5.010243\n",
      "562 batches of epoch 5 completed.  Last Training Loss:  6.285757\n",
      "572 batches of epoch 5 completed.  Last Training Loss:  6.631480\n",
      "582 batches of epoch 5 completed.  Last Training Loss:  6.228717\n",
      "592 batches of epoch 5 completed.  Last Training Loss:  6.823033\n",
      "Last Validation Loss:  7.720181, Lowest Validation Loss:  6.847796\n",
      "602 batches of epoch 5 completed.  Last Training Loss:  5.112241\n",
      "612 batches of epoch 5 completed.  Last Training Loss:  6.497499\n"
     ]
    }
   ],
   "source": [
    "# Changed validation output file name and made larger model.  Code should be right since can work\n",
    "# loss down below zero, but capacity not there.\n",
    "seqToseqNet = Seq2Seq(512,layer_count,pr_kv)\n",
    "train_model(seqToseqNet, \\\n",
    "            train_dataset_4,validation_dataset, \\\n",
    "            ses_lrn = 0.01, ses_tea = 0.5, ses_epochs = 5, ses_batch_size = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return inference given model, keyvectors vocabulary,prompt, maximum response length\n",
    "def return_inference(model,emb_kv,prompt,maxLen):\n",
    "    \n",
    "    # Bounded prompt\n",
    "    bounded_prompt = prompt + ' ' + eosToken\n",
    "    \n",
    "    # Convert prompt to embedding vectors\n",
    "    prompt_model_input = torch.tensor(tokensToIndices(prepare_text(bounded_prompt,emb_kv),emb_kv), dtype=torch.int64)\n",
    "    prompt_model_input = prompt_model_input.unsqueeze(dim=0)\n",
    "    prompt_question_lengths = torch.tensor([prompt_model_input.size(dim=1)], dtype=torch.int64)\n",
    "    \n",
    "    # Prepare unused target for forward method, except to feed in soseq token at start\n",
    "    unused_target = emb_kv.get_index(sosToken)*torch.ones(maxLen,dtype=torch.int64).unsqueeze(dim=0)\n",
    "    unused_lengths = torch.tensor([maxLen],dtype=torch.int64)\n",
    "        \n",
    "    # Get output tensor\n",
    "    output = model.to(\"cpu\")((prompt_model_input,prompt_question_lengths), \\\n",
    "                                     (unused_target,unused_lengths), \\\n",
    "                                     teacher_forcing_ratio=0)\n",
    "    outString = \"\"\n",
    "    \n",
    "    # Convert embeddings to words\n",
    "    for i in range(output.size(dim=1)):\n",
    "        next_word = emb_kv.index_to_key[torch.argmax(output[0][i])]\n",
    "        if next_word == eosToken:\n",
    "            outString += \" \" + next_word\n",
    "            break\n",
    "        else:\n",
    "            outString += \" \" + next_word\n",
    "    \n",
    "    return outString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the eoseq\n",
      " the eoseq\n",
      " the eoseq\n"
     ]
    }
   ],
   "source": [
    "lowest_val_model = torch.load(\"Min-Validation-Loss-Model.pt\")\n",
    "lowest_val_model.eval()\n",
    "\n",
    "prompt_one = \"What was the size of the notre dame endowment when theodore hesburgh became president?\"\n",
    "prompt_two = \"Tigers are nice but badgers are mean.  Which of these animals is kind?\"\n",
    "prompt_three = \"Some good foods for breakfast are pancakes, waffles, and french toast.  \" + \\\n",
    "               \"Some good foods for dinner are chicken, soup, and pizza.  Would it be \" + \\\n",
    "               \"better to serve pizza or pancakes tonight?\"\n",
    "\n",
    "for prompt in [prompt_one, prompt_two, prompt_three]:\n",
    "    print(return_inference(seqToseqNet,pr_kv,prompt,40))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
